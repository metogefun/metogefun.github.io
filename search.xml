<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[使用hexo和NexT搭建博客]]></title>
    <url>%2F2018%2F09%2F10%2Fnew-blog-theme%2F</url>
    <content type="text"><![CDATA[最近对blog进行了改版，采用hexo和NexT 评论系统使用valine 上面是两个东西的中文文档，按照教程操作还是比较快的。]]></content>
      <tags>
        <tag>hexo</tag>
        <tag>NexT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(转载）阿里巴巴为什么不用 ZooKeeper 做服务发现？]]></title>
    <url>%2F2018%2F06%2F13%2Fali-nonuse-zk%2F</url>
    <content type="text"><![CDATA[转载自infoq 站在未来的路口，回望历史的迷途，常常会很有意思，因为我们会不经意地兴起疯狂的念头，例如如果当年某事提前发生了，而另外一件事又没有发生会怎样？一如当年的奥匈帝国皇位继承人斐迪南大公夫妇如果没有被塞尔维亚族热血青年普林西普枪杀会怎样，又如若当年的丘老道没有经过牛家村会怎样？ 2008 年底，淘宝开启一个叫做“五彩石”的内部重构项目，这个项目后来成为了淘宝服务化、面向分布式走自研之路，走出了互联网中间件体系之始，而淘宝服务注册中心 ConfigServer 于同年诞生。 2008 年前后，Yahoo 这个曾经的互联网巨头开始逐渐在公开场合宣讲自己的大数据分布式协调产品 ZooKeeper，这个产品参考了 Google 发表的关于 Chubby 以及 Paxos 的论文。 2010 年 11 月，ZooKeeper 从 Apache Hadoop 的子项目发展为 Apache 的顶级项目，正式宣告 ZooKeeper 成为一个工业级的成熟稳定的产品。 2011 年，阿里巴巴开源 Dubbo，为了更好开源，需要剥离与阿里内部系统的关系，Dubbo 支持了开源的 ZooKeeper 作为其注册中心，后来在国内，在业界诸君的努力实践下，Dubbo + ZooKeeper 的典型的服务化方案成就了 ZooKeeper 作为注册中心的声名。 2015 年双 11，ConfigServer 服务内部近 8 个年头过去了，阿里巴巴内部“服务规模”超几百万 ，以及推进“千里之外”的 IDC 容灾技术战略等，共同促使阿里巴巴内部开启了 ConfigServer 2.0 到 ConfigServer 3.0 的架构升级之路。 时间走向 2018 年，站在 10 年的时间路口上，有多少人愿意在追逐日新月异的新潮技术概念的时候，稍微慢一下脚步，仔细凝视一下服务发现这个领域，有多少人想到过或者思考过一个问题： 服务发现，ZooKeeper 真的是最佳选择么？而回望历史，我们也偶有迷思，在服务发现这个场景下，如果当年 ZooKeeper 的诞生之日比我们 HSF 的注册中心 ConfigServer 早一点会怎样？ 我们会不会走向先使用 ZooKeeper 然后疯狂改造与修补 ZooKeeper 以适应阿里巴巴的服务化场景与需求的弯路？ 但是，站在今天和前人的肩膀上，我们从未如今天这样坚定的认知到，在服务发现领域，ZooKeeper 根本就不能算是最佳的选择，一如这些年一直与我们同行的 Eureka 以及这篇文章 《Eureka! Why You Shouldn’t Use ZooKeeper for Service Discovery》那坚定的阐述一样，为什么你不应该用 ZooKeeper 做服务发现！ 吾道不孤矣。 注册中心需求分析及关键设计考量接下来，让我们回归对服务发现的需求分析，结合阿里巴巴在关键场景上的实践，来一一分析，一起探讨为何说 ZooKeeper 并不是最合适的注册中心解决方案。 注册中心是 CP 还是 AP 系统?CAP 和 BASE 理论相信读者都已经耳熟能详，其业已成了指导分布式系统及互联网应用构建的关键原则之一，在此不再赘述其理论，我们直接进入对注册中心的数据一致性和可用性需求的分析: 数据一致性需求分析注册中心最本质的功能可以看成是一个 Query 函数 Si = F(service-name)，以 service-name 为查询参数，service-name 对应的服务的可用的 endpoints (ip:port)列表为返回值. 1注: 后文将 service 简写为 svc。 先来看看关键数据 endpoints (ip:port) 不一致性带来的影响，即 CAP 中的 C 不满足带来的后果 : 如上图所示，如果一个 svcB 部署了 10 个节点 (副本 /Replica），如果对于同一个服务名 svcB, 调用者 svcA 的 2 个节点的 2 次查询返回了不一致的数据，例如: S1 = { ip1,ip2,ip3…,ip9 }, S2 = { ip2,ip3,….ip10 }, 那么这次不一致带来的影响是什么？相信你一定已经看出来了，svcB 的各个节点流量会有一点不均衡。 ip1 和 ip10 相对其它 8 个节点{ip2…ip9}，请求流量小了一点，但很明显，在分布式系统中，即使是对等部署的服务，因为请求到达的时间，硬件的状态，操作系统的调度，虚拟机的 GC 等，任何一个时间点，这些对等部署的节点状态也不可能完全一致，而流量不一致的情况下，只要注册中心在 SLA 承诺的时间内（例如 1s 内）将数据收敛到一致状态（即满足最终一致），流量将很快趋于统计学意义上的一致，所以注册中心以最终一致的模型设计在生产实践中完全可以接受。 分区容忍及可用性需求分析接下来我们看一下网络分区（Network Partition）情况下注册中心不可用对服务调用产生的影响，即 CAP 中的 A 不满足时带来的影响。 考虑一个典型的 ZooKeeper 三机房容灾 5 节点部署结构 (即 2-2-1 结构)，如下图: 当机房 3 出现网络分区 (Network Partitioned) 的时候，即机房 3 在网络上成了孤岛，我们知道虽然整体 ZooKeeper 服务是可用的，但是节点 ZK5 是不可写的，因为联系不上 Leader。 也就是说，这时候机房 3 的应用服务 svcB 是不可以新部署，重新启动，扩容或者缩容的，但是站在网络和服务调用的角度看，机房 3 的 svcA 虽然无法调用机房 1 和机房 2 的 svcB, 但是与机房 3 的 svcB 之间的网络明明是 OK 的啊，为什么不让我调用本机房的服务？ 现在因为注册中心自身为了保脑裂 (P) 下的数据一致性（C）而放弃了可用性，导致了同机房的服务之间出现了无法调用，这是绝对不允许的！可以说在实践中，注册中心不能因为自身的任何原因破坏服务之间本身的可连通性，这是注册中心设计应该遵循的铁律！ 后面在注册中心客户端灾容上我们还会继续讨论。 同时我们再考虑一下这种情况下的数据不一致性，如果机房 1，2，3 之间都成了孤岛，那么如果每个机房的 svcA 都只拿到本机房的 svcB 的 ip 列表，也即在各机房 svcB 的 ip 列表数据完全不一致，影响是什么？ 其实没啥大影响，只是这种情况下，全都变成了同机房调用，我们在设计注册中心的时候，有时候甚至会主动利用这种注册中心的数据可以不一致性，来帮助应用主动做到同机房调用，从而优化服务调用链路 RT 的效果！ 通过以上我们的阐述可以看到，在 CAP 的权衡中，注册中心的可用性比数据强一致性更宝贵，所以整体设计更应该偏向 AP，而非 CP，数据不一致在可接受范围，而 P 下舍弃 A 却完全违反了注册中心不能因为自身的任何原因破坏服务本身的可连通性的原则。 服务规模、容量、服务联通性你所在公司的“微服务”规模有多大？数百微服务？部署了上百个节点？那么 3 年后呢？互联网是产生奇迹的地方，也许你的“服务”一夜之间就家喻户晓，流量倍增，规模翻番！ 当数据中心服务规模超过一定数量 (服务规模 =F{服务 pub 数, 服务 sub 数})，作为注册中心的 ZooKeeper 很快就会像下图的驴子一样不堪重负 其实当 ZooKeeper 用对地方时，即用在粗粒度分布式锁，分布式协调场景下，ZooKeeper 能支持的 tps 和支撑的连接数是足够用的，因为这些场景对于 ZooKeeper 的扩展性和容量诉求不是很强烈。 但在服务发现和健康监测场景下，随着服务规模的增大，无论是应用频繁发布时的服务注册带来的写请求，还是刷毫秒级的服务健康状态带来的写请求，还是恨不能整个数据中心的机器或者容器皆与注册中心有长连接带来的连接压力上，ZooKeeper 很快就会力不从心，而 ZooKeeper 的写并不是可扩展的，不可以通过加节点解决水平扩展性问题。 要想在 ZooKeeper 基础上硬着头皮解决服务规模的增长问题，一个实践中可以考虑的方法是想办法梳理业务，垂直划分业务域，将其划分到多个 ZooKeeper 注册中心，但是作为提供通用服务的平台机构组，因自己提供的服务能力不足要业务按照技术的指挥棒配合划分治理业务，真的可行么？ 而且这又违反了因为注册中心自身的原因（能力不足）破坏了服务的可连通性，举个简单的例子，1 个搜索业务，1 个地图业务，1 个大文娱业务，1 个游戏业务，他们之间的服务就应该老死不相往来么？也许今天是肯定的，那么明天呢，1 年后呢，10 年后呢？谁知道未来会要打通几个业务域去做什么奇葩的业务创新？注册中心作为基础服务，无法预料未来的时候当然不能妨碍业务服务对未来固有联通性的需求。 注册中心需要持久存储和事务日志么？需要，也不需要。 我们知道 ZooKeeper 的 ZAB 协议对每一个写请求，会在每个 ZooKeeper 节点上保持写一个事务日志，同时再加上定期的将内存数据镜像（Snapshot）到磁盘来保证数据的一致性和持久性，以及宕机之后的数据可恢复，这是非常好的特性，但是我们要问，在服务发现场景中，其最核心的数据 - 实时的健康的服务的地址列表真的需要数据持久化么？ 对于这份数据，答案是否定的。 如上图所示，如果 svcB 经历了注册服务 (ip1) 到扩容到 2 个节点（ip1，ip2）到因宕机缩容 (ip1 宕机），这个过程中，产生了 3 次针对 ZooKeeper 的写操作。 但是仔细分析，通过事务日志，持久化连续记录这个变化过程其实意义不大，因为在服务发现中，服务调用发起方更关注的是其要调用的服务的实时的地址列表和实时健康状态，每次发起调用时，并不关心要调用的服务的历史服务地址列表、过去的健康状态。 但是为什么又说需要呢，因为一个完整的生产可用的注册中心，除了服务的实时地址列表以及实时的健康状态之外，还会存储一些服务的元数据信息，例如服务的版本，分组，所在的数据中心，权重，鉴权策略信息，service label 等元信息，这些数据需要持久化存储，并且注册中心应该提供对这些元信息的检索的能力。 Service Health Check使用 ZooKeeper 作为服务注册中心时，服务的健康检测常利用 ZooKeeper 的 Session 活性 Track 机制 以及结合 Ephemeral ZNode 的机制，简单而言，就是将服务的健康监测绑定在了 ZooKeeper 对于 Session 的健康监测上，或者说绑定在 TCP 长链接活性探测上了。 这在很多时候也会造成致命的问题，ZK 与服务提供者机器之间的 TCP 长链接活性探测正常的时候，该服务就是健康的么？答案当然是否定的！注册中心应该提供更丰富的健康监测方案，服务的健康与否的逻辑应该开放给服务提供方自己定义，而不是一刀切搞成了 TCP 活性检测！ 健康检测的一大基本设计原则就是尽可能真实的反馈服务本身的真实健康状态，否则一个不敢被服务调用者相信的健康状态判定结果还不如没有健康检测。 注册中心的容灾考虑前文提过，在实践中，注册中心不能因为自身的任何原因破坏服务之间本身的可连通性，那么在可用性上，一个本质的问题，如果注册中心（Registry）本身完全宕机了，svcA 调用 svcB 链路应该受到影响么？ 是的，不应该受到影响。 服务调用（请求响应流）链路应该是弱依赖注册中心，必须仅在服务发布，机器上下线，服务扩缩容等必要时才依赖注册中心。 这需要注册中心仔细的设计自己提供的客户端，客户端中应该有针对注册中心服务完全不可用时做容灾的手段，例如设计客户端缓存数据机制（我们称之为 client snapshot）就是行之有效的手段。另外，注册中心的 health check 机制也要仔细设计以便在这种情况不会出现诸如推空等情况的出现。 ZooKeeper 的原生客户端并没有这种能力，所以利用 ZooKeeper 实现注册中心的时候我们一定要问自己，如果把 ZooKeeper 所有节点全干掉，你生产上的所有服务调用链路能不受任何影响么？而且应该定期就这一点做故障演练。 你有没有 ZooKeeper 的专家可依靠？ZooKeeper 看似很简单的一个产品，但在生产上大规模使用并且用好，并不是那么理所当然的事情。如果你决定在生产中引入 ZooKeeper，你最好做好随时向 ZooKeeper 技术专家寻求帮助的心理预期，最典型的表现是在两个方面: 难以掌握的 Client/Session 状态机ZooKeeper 的原生客户端绝对称不上好用，Curator 会好一点，但其实也好的有限，要完全理解 ZooKeeper 客户端与 Server 之间的交互协议也并不简单，完全理解并掌握 ZooKeeper Client/Session 的状态机（下图）也并不是那么简单明了: 但基于 ZooKeeper 的服务发现方案却是依赖 ZooKeeper 提供的长连接 /Session 管理，Ephemeral ZNode，Event&amp;Notification, ping 机制上，所以要用好 ZooKeeper 做服务发现，恰恰要理解这些 ZooKeeper 核心的机制原理，这有时候会让你陷入暴躁，我只是想要个服务发现而已，怎么要知道这么多？而如果这些你都理解了并且不踩坑，恭喜你，你已经成为 ZooKeeper 的技术专家了。 难以承受的异常处理我们在阿里巴巴内部应用接入 ZooKeeper 时，有一个《ZooKeeper 应用接入必知必会》的 WIKI，其中关于异常处理有过如下的论述: 如果说要选出应用开发者在使用 ZooKeeper 的过程中，最需要了解清楚的事情？那么根据我们之前的支持经验，一定是异常处理。 当所有一切（宿主机，磁盘，网络等等）都很幸运的正常工作的时候，应用与 ZooKeeper 可能也会运行的很好，但不幸的是，我们整天会面对各种意外，而且这遵循墨菲定律，意料之外的坏事情总是在你最担心的时候发生。 所以务必仔细了解 ZooKeeper 在一些场景下会出现的异常和错误，确保您正确的理解了这些异常和错误，以及知道您的应用如何正确的处理这些情况。 ConnectionLossException 和 Disconnected 事件 简单来说，这是个可以在同一个 ZooKeeper Session 恢复的异常 (Recoverable), 但是应用开发者需要负责将应用恢复到正确的状态。 发生这个异常的原因有很多，例如应用机器与 ZooKeeper 节点之间网络闪断，ZooKeeper 节点宕机，服务端 Full GC 时间超长，甚至你的应用进程 Hang 死，应用进程 Full GC 时间超长之后恢复都有可能。 要理解这个异常，需要了解分布式应用中的一个典型的问题，如下图： 在一个典型的客户端请求、服务端响应中，当它们之间的长连接闪断的时候，客户端感知到这个闪断事件的时候，会处在一个比较尴尬的境地，那就是无法确定该事件发生时附近的那个请求到底处在什么状态，Server 端到底收到这个请求了么？已经处理了么？因为无法确定这一点，所以当客户端重新连接上 Server 之后，这个请求是否应该重试（Retry）就也要打一个问号。 所以在处理连接断开事件中，应用开发者必须清楚处于闪断附近的那个请求是什么（这常常难以判断），该请求是否是幂等的，对于业务请求在 Server 端服务处理上对于&quot;仅处理一次&quot; &quot;最多处理一次&quot; &quot;最少处理一次&quot;语义要有选择和预期。 举个例子，如果应用在收到 ConnectionLossException 时，之前的请求是 Create 操作，那么应用的 catch 到这个异常，应用一个可能的恢复逻辑就是，判断之前请求创建的节点的是否已经存在了，如果存在就不要再创建了，否则就创建。 再比如，如果应用使用了 exists Watch 去监听一个不存在的节点的创建的事件，那么在 ConnectionLossException 的期间，有可能遇到的情况是，在这个闪断期间，其它的客户端进程可能已经创建了节点，并且又已经删除了，那么对于当前应用来说，就 miss 了一次关心的节点的创建事件，这种 miss 对应用的影响是什么？是可以忍受的还是不可接受？需要应用开发者自己根据业务语义去评估和处理。 SessionExpiredException 和 SessionExpired 事件 Session 超时是一个不可恢复的异常，这是指应用 Catch 到这个异常的时候，应用不可能在同一个 Session 中恢复应用状态，必须要重新建立新 Session，老 Session 关联的临时节点也可能已经失效，拥有的锁可能已经失效。... 我们阿里巴巴的小伙伴在自行尝试使用 ZooKeeper 做服务发现的过程中，曾经在我们的内网技术论坛上总结过一篇自己踩坑的经验分享 在该文中中肯的提到: … 在编码过程中发现很多可能存在的陷阱，毛估估，第一次使用 zk 来实现集群管理的人应该有 80% 以上会掉坑，有些坑比较隐蔽，在网络问题或者异常的场景时才会出现，可能很长一段时间才会暴露出来 … 这篇文章已经分享到云栖社区, 你可以点击 这里 详细阅读。 #向左走，向右走 阿里巴巴是不是完全没有使用 ZooKeeper？并不是！ 熟悉阿里巴巴技术体系的都知道，其实阿里巴巴维护了目前国内乃至世界上最大规模的 ZooKeeper 集群，整体规模有近千台的 ZooKeeper 服务节点。 同时阿里巴巴中间件内部也维护了一个面向大规模生产的、高可用、更易监控和运维的 ZooKeeper 的代码分支 TaoKeeper，如果以我们近 10 年在各个业务线和生产上使用 ZooKeeper 的实践，给 ZooKeeper 用一个短语评价的话，那么我们认为 ZooKeeper 应该是 “The King Of Coordination for Big Data”！ 在粗粒度分布式锁，分布式选主，主备高可用切换等不需要高 TPS 支持的场景下有不可替代的作用，而这些需求往往多集中在大数据、离线任务等相关的业务领域，因为大数据领域，讲究分割数据集，并且大部分时间分任务多进程 / 线程并行处理这些数据集，但是总是有一些点上需要将这些任务和进程统一协调，这时候就是 ZooKeeper 发挥巨大作用的用武之地。 但是在交易场景交易链路上，在主业务数据存取，大规模服务发现、大规模健康监测等方面有天然的短板，应该竭力避免在这些场景下引入 ZooKeeper，在阿里巴巴的生产实践中，应用对 ZooKeeper 申请使用的时候要进行严格的场景、容量、SLA 需求的评估。 所以可以使用 ZooKeeper，但是大数据请向左，而交易则向右，分布式协调向左，服务发现向右。 结语感谢你耐心的阅读到这里，至此，我相信你已经理解，我们写这篇文章并不是全盘否定 ZooKeeper，而只是根据我们阿里巴巴近 10 年来在大规模服务化上的生产实践，对我们在服务发现和注册中心设计及使用上的经验教训进行一个总结，希望对业界就如何更好的使用 ZooKeeper，如何更好的设计自己的服务注册中心有所启发和帮助。 最后，条条大路通罗马，衷心祝愿你的注册中心直接就诞生在罗马。 参考文章[1] https://medium.com/knerd/eureka-why-you-shouldnt-use-zookeeper-for-service-discovery-4932c5c7e764[2] https://yq.aliyun.com/articles/227260]]></content>
      <categories>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
        <tag>服务发现</tag>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mysql InnoDB 锁]]></title>
    <url>%2F2018%2F03%2F08%2Fmysql-lock%2F</url>
    <content type="text"><![CDATA[本篇阐述mysql InnoDB引擎锁分类以及相关知识 共享锁和排它锁我们都知道InnoDB实现了行级锁，行级锁分为两类：共享和排他。 共享锁(Sharded Lock, S)允许持有锁的事务读取行 排它锁(Exclusive Lock, X)允许持有锁的事务更新或删除行 顾名思义，共享锁可以在多个事务中共享，例如事务T1在行r获取一个共享锁，另外一个事务T2再次在行r上请求共享锁，会立马获取到。这样T1和T2都持有在行r上的共享锁。 如果t2请求在行r上的排它锁，将不能立即获取到。反之如果T1获取r上的排它锁，则后续不管T2是什么类型的锁，都不会立即获取。 123456789101112131415161718192021222324252627282930313233343536--现在创建一张简单的表+-------+-------------+------+-----+---------+-------+| Field | Type | Null | Key | Default | Extra |+-------+-------------+------+-----+---------+-------+| id | int(11) | NO | PRI | NULL | || name | varchar(20) | YES | | NULL | |+-------+-------------+------+-----+---------+-------+--插入数据insert into user values(1, 'zhao'), (2, 'qian'), (3, 'sun'), (4, 'li'), (8, 'zhou'), (12, 'wu');--我们现在一个mysql客户端执行下面语句来获取共享锁：START TRANSACTION;SELECT * FROM `user` WHERE id = 1 LOCK IN SHARE MODE;+----+------+| id | name |+----+------+| 1 | zhao |+----+------+--打开另一个客户端，执行与上面相同语句：START TRANSACTION;SELECT * FROM `user` WHERE id = 1 LOCK IN SHARE MODE;+----+------+| id | name |+----+------+| 1 | zhao |+----+------+--我们看到在另一个客户端我们也获取到了共享锁，下面我们试试获取排它锁START TRANSACTION;SELECT * FROM `user` WHERE id = 1 for update;ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction--我们并没有获取到相应的排它锁。 意图锁InnoDB支持多个粒度锁，允许行锁和表锁共存。为了实现多个粒度级别的锁，InnoDB使用意图锁。意图锁是表级锁定，用来表明事务稍后对表中的行所需的锁类型（共享或独占）。意图锁分为两种： 意图共享锁（IS）表明事务打算在表中的个别行设置共享锁。 意图排他锁（IX）表示事务打算在表中的个别行上设置独占锁。 在事务可以获取表中某行的S锁之前，它必须首先在表上获取IS或更强的锁。同样在事务可以获取表中某行的X锁之前，它必须首先获取IX锁。 意图锁定不会阻止除表的X锁（例如，LOCK TABLES … WRITE）请求之外的任何内容。意图锁的主要目的是表明事务将要锁定表中的行。 表级锁的的兼容性如下，可以将列想象为事务T1已获取到的表级锁，行为事务T2要请求的表锁。 X IX S IS X 冲突 冲突 冲突 冲突 IX 冲突 兼容 冲突 兼容 S 冲突 冲突 兼容 兼容 IS 冲突 兼容 兼容 兼容 如果锁与现有锁兼容，则向请求事务授予锁，但如果它与现有锁冲突，则不授予锁。事务等待直到冲突的现有锁被释放。 记录锁(Record Lock)记录锁是索引记录上的锁。InnoDB的行锁实现方式是锁定索引记录，而不是行数据。例如，SELECT c1 FROM t WHERE c1 = 10 FOR UPDATE; 防止任何其他事务插入，更新或删除t.c1的值为10的行。 如果一个查询条件没有索引，则将会使用表锁。 12345678910111213START TRANSACTION;select * from user where name = 'zhao' for update;+----+------+| id | name |+----+------+| 1 | zhao |+----+------+START TRANSACTION;SELECT * FROM `user` WHERE id = 1 for update;ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction 以上例子中，第一个事务占有name=’zhao’的排它锁，但是第二个事务却无法获取到id=1的排它锁。所以id=1也被第一个事务锁定，造成这个问题的原因因为没有在name上没有索引存在，会进行全表扫描，所以就对整张表进行锁定。 间隙锁(Gap Lock)间隙锁是锁定索引记录之间的间隙，或锁定在第一个或最后一个索引记录之前的间隙上。例如，SELECT c1 FROM t WHERE c1 BETWEEN 10 AND 20 FOR UPDATE; 阻止其他事务将值15插入到列t.c1中，无论列中是否存在任何此类值，因为该范围内所有现有值之间的间隙都被锁定。 间隙可能跨越单个索引值，多个索引值，甚至可能为空。 间隙锁是性能和并发之间权衡的一部分，仅用于某些事务隔离级别。 使用唯一索引查找唯一行不需要间隙锁。这不包括搜索条件仅包括多列唯一索引的一些列的情况; 在这种情况下，确实会发生间隙锁定。例如，如果id列有唯一的索引，下面的语句只对id值为100的行使用索引记录锁，而其他会话是否在前面的间隙中插入行并不重要: 1SELECT * FROM child WHERE id = 100; 如果ID未被索引或具有非唯一索引，则语句确实锁定前面的间隙。 123456789101112131415START TRANSACTION;-- 锁住4-12的间隙select * from user where id between 4 and 12 for update;+----+------+------+| id | name | age |+----+------+------+| 4 | li | 18 || 8 | zhou | 40 || 12 | wu | 52 |+----+------+------+-- 阻止插7的记录insert into user values (7, 'wang', 23);ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction 这里也值得注意的是，不同的事务可以在间隙上持有冲突锁。例如，事务A可以在间隙上持有共享间隙锁(gap S-lock)，而事务B可以在相同的间隙上持有独占间隙锁(gap X-lock)。允许冲突间隙锁定的原因是，如果从索引中清除记录，则必须合并由不同事务保留在记录上的间隙锁定。 注:上述所描述的不通事务可以在相同的间隙上持有冲突锁。没有验证成功。如果有幸能看到这篇文章，知道原因可留言解答。 InnoDB中的间隙锁是“纯粹的抑制”，这意味着它们只能阻止其他事务插入间隙。它们不会阻止不同的事务在同一间隙上进行间隙锁定。 因此，间隙X锁具有与间隙S锁相同的效果。 可以显式禁用间隙锁。如果将事务隔离级别更改为READ COMMITTED或启用innodb_locks_unsafe_for_binlog系统变量（现已弃用），则会发生这种情况。在这些情况下，对于搜索和索引扫描禁用间隙锁定，并且仅用于外键约束检查和重复键检查。 使用READ COMMITTED隔离级别或启用innodb_locks_unsafe_for_binlog还有其他影响。MySQL评估了WHERE条件后，将释放不匹配行的记录锁。对于UPDATE语句，InnoDB执行“半一致”读取，以便将最新提交的版本返回给MySQL，以便MySQL可以确定该行是否与UPDATE的WHERE条件匹配。 Next-Key锁Next-Key锁是索引记录上的记录锁和索引记录之前的间隙上的间隙锁的组合。 InnoDB以这样的方式执行行级锁定：当它搜索或扫描表索引时，它会在遇到的索引记录上设置共享锁或排它锁。因此，行级锁实际上是索引记录锁。索引记录上的Next-key锁也会影响该索引记录之前的“间隙”。也就是说，Next-Key锁是索引记录锁加上索引记录之前的间隙上的间隙锁。如果一个会话在索引中的记录R上具有共享锁或独占锁，则另一个会话不能在索引顺序中的R之前的间隙中插入新的索引记录。 假设索引包含值10,11,13和20。此索引的可能的Next-Key锁包括以下间隔，其中圆括号表示区间终点的排除，方括号表示包含端点： 12345(negative infinity, 10](10, 11](11, 13](13, 20](20, positive infinity) 对于最后一个间隔，next-key锁锁定索引中最大值以上的间隔和“上界”伪记录，其值大于索引中实际的任何值。上界不是一个真正的索引记录，因此，实际上，这个next-key锁只锁定最大索引值之后的间隙。 默认情况下，InnoDB在可重复读事务隔离级别运行。在这种情况下，InnoDB使用next-key锁进行搜索和索引扫描，这可以防止幽灵行。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758--修改上表，添加age字段，并将其指定为普通索引。select * from user;+----+------+------+| id | name | age |+----+------+------+| 6 | zhao | 20 || 8 | li | 35 || 10 | zhou | 65 || 12 | wu | 35 || 16 | wang | 19 || 20 | qian | 65 || 30 | sun | 35 || 40 | wang | 10 || 50 | feng | 43 |+----+------+------+START TRANSACTION;SELECT * FROM `user` WHERE age = 35 FOR UPDATE;-- 按age升序排列：+----+------+------+| id | name | age |+----+------+------+| 40 | wang | 10 || 16 | wang | 19 || 6 | zhao | 20 || 8 | li | 35 || 12 | wu | 35 || 30 | sun | 35 || 50 | feng | 43 || 10 | zhou | 65 || 20 | qian | 65 |+----+------+------+可以看到锁的间隙是20-43，所以插入该范围的数据需要等待锁，下面是执行插入的结果：INSERT INTO `user` VALUES(31, 'wang', 21);ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transactionINSERT INTO `user` VALUES(32, 'wang', 42);ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transactionINSERT INTO `user` VALUES(32, 'wang', 44);Query OK, 1 row affected (0.01 sec)INSERT INTO `user` VALUES(33, 'wang', 15);Query OK, 1 row affected (0.01 sec)INSERT INTO `user` VALUES(34, 'wang', 20);ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transactionINSERT INTO `user` VALUES(4, 'wang', 20);Query OK, 1 row affected (0.02 sec)INSERT INTO `user` VALUES(36, 'wang', 43);ERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transactionINSERT INTO `user` VALUES(51, 'wang', 43);Query OK, 1 row affected (0.01 sec)无法插入&gt;=21和&lt;=42的值，因为间隙被锁定。如果不在该范围自由插入。如果插入的数据为，20和42，我们可以看到age为20最大的id为6，则如果插入的age为20，并且id &gt; 6则无法插入，&lt; 6可以插入。age为43最小id是50，则插入age为20，并且id &gt; 50可以插入，&lt; 50无法插入。 插入意图锁插入意图锁是在行插入之前由INSERT操作设置的一种间隙锁。该锁表示以这样的方式插入的意图：如果插入到相同索引间隙中的多个事务不插入间隙内的相同位置，则不需要等待彼此。假设存在值为4和7的索引记录。尝试分别插入值5和6的单独事务，在获取插入行上的独占锁之前，每个用插入意图锁锁定4和7之间的间隙，但不会因为行不冲突而相互阻塞。 下面的示例演示了一个事务，在获取插入记录上的独占锁之前使用插入意图锁。这个示例涉及两个客户端A和B。 客户端A创建了一个包含两个索引记录(90和102)的表，然后启动一个事务，在ID大于100的索引记录上放置一个独占锁。独占锁包括记录102之前的间隙锁定： 12START TRANSACTION;SELECT * FROM user WHERE id &gt; 30 FOR UPDATE; 客户端B开始一个事务以将记录插入间隙。 该事务在等待获取独占锁时采用插入意图锁。 12START TRANSACTION;INSERT INTO child (id) VALUES (101); 客户端B开始一个事务以将记录插入间隙。 该事务在等待获取独占锁时采用插入意图锁。 AUTO-INC锁AUTO-INC锁是由插入到具有AUTO_INCREMENT列的表中的事务所采用的特殊表级锁。在最简单的情况下，如果一个事务正在向表中插入值，则任何其他事务必须等待对该表执行自己的插入，以便第一个事务插入的行接收连续的主键值。 innodb_autoinc_lock_mode配置选项控制用于自动增量锁定的算法。它允许您选择如何在可预测的自动增量值序列和插入操作的最大并发之间进行权衡。 空间索引的谓词锁InnoDB支持对包含空间列的列进行空间索引. 为了处理涉及空间索引的操作的锁定，next-key锁不能很好地支持可重复读或可序列化的事务隔离级别。多维数据中没有绝对的排序概念，因此不清楚哪个是“next” key。 为了支持具有空间索引的表的隔离级别，InnoDB使用谓词锁。空间索引包含最小包围矩形(MBR)值，因此InnoDB通过在用于查询的MBR值上设置谓词锁来强制索引上的一致性读取。其他事务不能插入或修改与查询条件匹配的行。 总结InnoDB的锁都是基于索引的，InnoDB是对索引记录加锁，而不是行数据。如果是查询唯一索引，间隙锁只会锁住查询范围内的那些索引记录，而如果查询的是非唯一索引，则innoDB会采用Next-key Locking，不仅会锁定查询范围那些记录，还会锁定该范围之前和之后的记录。]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>Lock</tag>
        <tag>InnoDB</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[消息系统]]></title>
    <url>%2F2018%2F03%2F01%2Finternal-message-system%2F</url>
    <content type="text"><![CDATA[年后对目前系统存在的问题现状，做了一次深入的分析，将已经存在以及应对未来的需求做了些规划。之前确实挖的坑实在是太多，整个分析下来要做的有十几个大项，大部分是关于架构、数据、规范、服务化等方面。估计要做下来怎么也得一两年。。。真正做到了前人挖坑，后人填，如果这次没能做好前期架构，以及严格保证开发过程，可能也会造成”后人哀之而不鉴之，亦使后人而复哀后人也“的局面。 其中有一项算是服务拆分相关，抽取关键的公共服务。消息系统算是其中之一。目前内部对于系统的要求场景比较单一，主要就是短信和钉钉两种。谈起设计起始感觉也有点老生常谈，平时我们听过的设计方案，基本就足以支持现在的业务量。更多的还是要关注细节。下图是整个系统的描述： 整体相对简单，在这就坐下简单的介绍： message-gateway 消息网关。通过暴露统一的restapi方式，为内部提供消息发送功能。服务收到消息会在本地持久化，发送MQ消息，然后返回给客户端。并在之后处理消息响应以及回调改变消息状态。 outside-gateway 外部网关。一般接入第三方消息系统都会有回调机制，该服务为了处理第三方回调，然后将结果发送到mq由message-gateway处理。 消息服务，剩下的就是跟第三方消息系统对接的服务。为了方便扩展，可能跟每个第三方服务都会有一个接入 系统的核心是message-gateway服务，所以保证该服务稳定至关重要，在此引入eureka作为注册中心，用于服务发现和客户端负载。因为内部多为dubbo服务，采用zk作为注册中心，所以还需要为使用方提供访问eureka的帮助类。 关于为什么选用eureka，之前也是看过一篇相关的文章Eureka! Why You Shouldn’t Use ZooKeeper for Service Discovery。但是总体来说对于我们目前的量级，用什么基本感知一样，但是有一个确实比较严重的问题就是jar包依赖。使用dubbo一般会暴露服务的api包给使用方，但是这就要求有一个合理的版本升级机制，但是。。。目前比较混乱，也没有所谓的版本升级。经常发现线上各种SNAPSHOT版本，新添加的功能直接在原来版本上改，然后直接deploy，线上使用同一maven私服，当部署线上时直接使用了最新的SNAPSHOT。经常会有一些接口不兼容的升级。。。所以引入rest也是想从源头杜绝这种情况。 message-gateway提供了签名验证机制。使用的业务方接入要分配一个账号、秘钥，在正常传入请求参数的同时，还需要提供一个根据参数列表，以及秘钥进行MD5加密后的签名。同时服务端提供拦截器进行拦截验证安全性。 对于消息的附加了一系列的过滤链，提供了关于幂等性验证、账号黑名单、ip黑名单以及下发ip白名单的过滤机制。这里说下幂等性，目前实现比较简单，主要是针对发送内容、账号做了MD5加密后作为key存入redis，策略是对于验证码类的短信，简单的做下过期。 关于消息持久化和发mq的事务保证。使用了spring的@TransactionalEventListener事件机制，在事务成功提交后回调该事件。同时还需要提供补偿机制，轮询表中（目前最简单的方式）一段时间内未发送mq成功的消息，进行重试。在多个message-gateway服务下，同时执行定时任务会存在问题，可以使用主备、或者分片方案，例如只让某一个服务进行轮询，其他服务作为备机，如果主挂了之后，再由备的去争夺轮询权限。也可以所有服务都可以轮询，将数据按照服务的数量分片。 关于消息通道切换的问题，例如服务商商A挂掉后，可以动态的切到服务商B，也可以人为的切换通道。例如，A作为短信主通道，当通过A发送消息后，出现网络异常、或者单个用户超上限后，可以做整条通道切换和单用户通道切换。由于网络问题发生切换后，需要通过重试去验证通道是否恢复，待检测到恢复后需要切回。针对整条通道的切换分为主动和被动。主动是人为切换，所以不去做重试检测，如果为由于网络异常被动切换，则可以选举一个服务做重试检测。用户通道切换可以放在redis，第二天过期即可。 outside-gateway主要接受第三方回调，然后将回调信息丢到mq由message-gateway处理。如果在丢到mq失败的情况下也不要紧，会有补偿机制主动去查询。 以上就是消息系统的大体设计思路。最近也在研究DDD，代码部分原遵照DDD去编写，不过我对DDD的理解还是比较浅显，可能有些不甚合理，所以将部分代码放在此。有兴趣可以一起交流下。]]></content>
      <categories>
        <category>实例</category>
      </categories>
      <tags>
        <tag>架构</tag>
        <tag>消息</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[基于dubbo SPI实现trace]]></title>
    <url>%2F2018%2F01%2F19%2Fdubbo-spi-trace%2F</url>
    <content type="text"><![CDATA[随着公司业务增长，系统间交互变得频繁，由于目前公司架构以及基础设施都比较差bug不断，通常需要多个系统共同排查问题。对于跨系统定位bug问题，由于没有一个统一的traceId，很难将一个业务线的请求跟另一个业务线的请求关联起来。于是花了点时间，写了一套简单的解决方案，目前也在向各业务线进行推广。 因为内部系统使用的Dubbo作为RPC框架，该方案也是针对dubbo。代码地址：https://github.com/metogefun/shadow 设计之初最优先考虑的就是零侵入。为了使业务线更方便接入，所有一切都为了给业务线节省成本。该方案主要实用了Dubbo的SPI机制扩展了Filter，自定义两个不同的Filter，TraceProviderFilter和TraceConsumerFilter。实现的功能也很简单，从代码就能看出来做了什么。 TraceConsumerFilter，从TraceContext中获取已经放入的TRACE_ID，对使用次数进行+1操作，并将TRACE_ID作为附加参数传递到Dubbo Provider端，并在操作执行完成后进行释放，也就是-1。当为使用次数为0时，清除TraceContext。还有一个关键问题就是TRACE_ID是怎么初始化的呢？首先一个调用开始，肯定有一个主动出发的事件，大多数是用户的操作，也就是从web项目提供的接口开始，这时候需要在web程序中加一个拦截器，初始化TraceContext放入一个UUID生成的TRACE_ID，这个TRACE_ID存在ThreadLocal中方式并发存在的问题。 TraceProviderFilter，在附加参数中获取TRACE_ID，并初始化它自己的TraceContext，如果该dubbo服务需要调用其他dubbo服务，需要将该TraceID传递下去，当然方法执行完成后，TRACE_ID也会进行销毁。 这样整个调用链就可以绑定到同一个TraceId上，解决了我们开始提出的问题。后面也可以针对该TraceID做一些调用时长的监控，不过有很多解决方案，例如CAT、Pinpoint等，不过最简单的方案还是Pinpoint通过JavaAgent，不侵入代码，但是对性能会有影响。目前也在测试环境搭建了pinpoint环境很方便，并且针对于报警的二次开发很简单，也是推荐在没有成熟的调用链监控方案上，pinpoint不失为一种快捷有效的方式。]]></content>
      <categories>
        <category>dubbo</category>
        <category>实例</category>
      </categories>
      <tags>
        <tag>dubbo</tag>
        <tag>trace</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[(转载）实例详解ZooKeeper ZAB协议、分布式锁与领导选举]]></title>
    <url>%2F2018%2F01%2F10%2Fzk-zab%2F</url>
    <content type="text"><![CDATA[本文转载自原地址 本节将介绍ZooKeeper的架构，并结合实例分析原子广播(ZAB)协议的原理，包括但不限于ZooKeeper的读写流程，FastLeaderElection算法的原理，ZAB如何保证Leader Failover过程中的数据一致性。 ZooKeeper是什么ZooKeeper是一个分布式协调服务，可用于服务发现、分布式锁、分布式领导选举、配置管理等。 这一切的基础，都是ZooKeeper提供了一个类似于Linux文件系统的树形结构（可认为是轻量级的内存文件系统，但只适合存少量信息，完全不适合存储大量文件或者大文件），同时提供了对于每个节点的监控与通知机制。 既然是一个文件系统，就不得不提ZooKeeper是如何保证数据的一致性的。本节将将介绍ZooKeeper如何保证数据一致性，如何进行领导选举，以及数据监控/通知机制的语义保证。 ZooKeeper服务器角色ZooKeeper集群是一个基于主从复制的高可用集群，每个服务器承担如下三种角色中的一种： Leader 一个ZooKeeper集群同一时间只会有一个实际工作的Leader，它会发起并维护与各Follwer及Observer间的心跳。所有的写操作必须要通过Leader完成再由Leader将写操作广播给其它服务器。 Follower 一个ZooKeeper集群可能同时存在多个Follower，它会响应Leader的心跳。Follower可直接处理并返回客户端的读请求，同时会将写请求转发给Leader处理，并且负责在Leader处理写请求时对请求进行投票。 Observer 角色与Follower类似，但是无投票权。 原子广播（ZAB）为了保证写操作的一致性与可用性，ZooKeeper专门设计了一种名为原子广播（ZAB）的支持崩溃恢复的一致性协议。基于该协议，ZooKeeper实现了一种主从模式的系统架构来保持集群中各个副本之间的数据一致性。 根据ZAB协议，所有的写操作都必须通过Leader完成，Leader写入本地日志后再复制到所有的Follower节点。 一旦Leader节点无法工作，ZAB协议能够自动从Follower节点中重新选出一个合适的替代者，即新的Leader，该过程即为领导选举。该领导选举过程，是ZAB协议中最为重要和复杂的过程。 1、写Leader通过Leader进行写操作流程如下图所示： 由上图可见，通过Leader进行写操作，主要分为五步： 客户端向Leader发起写请求 Leader将写请求以Proposal的形式发给所有Follower并等待ACK Follower收到Leader的Proposal后返回ACK Leader得到过半数的ACK（Leader对自己默认有一个ACK）后向所有的Follower和Observer发送Commmit Leader将处理结果返回给客户端 这里要注意： Leader并不需要得到Observer的ACK，即Observer无投票权 Leader不需要得到所有Follower的ACK，只要收到过半的ACK即可，同时Leader本身对自己有一个ACK。上图中有4个Follower，只需其中两个返回ACK即可，因为(2+1) / (4+1) &gt; 1/2 Observer虽然无投票权，但仍须同步Leader的数据从而在处理读请求时可以返回尽可能新的数据 2、写Follower/Observer通过Follower/Observer进行写操作流程如下图所示： 从上图可见： Follower/Observer均可接受写请求，但不能直接处理，而需要将写请求转发给Leader处理 除了多了一步请求转发，其它流程与直接写Leader无任何区别 3、读操作Leader/Follower/Observer都可直接处理读请求，从本地内存中读取数据并返回给客户端即可。 由于处理读请求不需要服务器之间的交互，Follower/Observer越多，整体可处理的读请求量越大，也即读性能越好。 支持的领导选举算法可通过electionAlg配置项设置ZooKeeper用于领导选举的算法。到3.4.10版本为止，可选项有： 0 基于UDP的LeaderElection 1 基于UDP的FastLeaderElection 2 基于UDP和认证的FastLeaderElection 3 基于TCP的FastLeaderElection 在3.4.10版本中，默认值为3，也即基于TCP的FastLeaderElection。另外三种算法已经被弃用，并且有计划在之后的版本中将它们彻底删除而不再支持。 FastLeaderElection原理1、myid每个ZooKeeper服务器，都需要在数据文件夹下创建一个名为myid的文件，该文件包含整个ZooKeeper集群唯一的ID（整数）。例如，某ZooKeeper集群包含三台服务器，hostname分别为zoo1、zoo2和zoo3，其myid分别为1、2和3，则在配置文件中其ID与hostname必须一一对应，如下所示。在该配置文件中，server.后面的数据即为myid 123server.1=zoo1:2888:3888server.2=zoo2:2888:3888server.3=zoo3:2888:3888 2、zxid类似于RDBMS中的事务ID，用于标识一次更新操作的Proposal ID。为了保证顺序性，该zkid必须单调递增。因此ZooKeeper使用一个64位的数来表示，高32位是Leader的epoch，从1开始，每次选出新的Leader，epoch加一。低32位为该epoch内的序号，每次epoch变化，都将低32位的序号重置。这样保证了zkid的全局递增性。 3、服务器状态 LOOKING 不确定Leader状态。该状态下的服务器认为当前集群中没有Leader，会发起Leader选举。 FOLLOWING 跟随者状态。表明当前服务器角色是Follower，并且它知道Leader是谁。 LEADING 领导者状态。表明当前服务器角色是Leader，它会维护与Follower间的心跳。 OBSERVING 观察者状态。表明当前服务器角色是Observer，与Folower唯一的不同在于不参与选举，也不参与集群写操作时的投票。 4、选票数据结构每个服务器在进行领导选举时，会发送如下关键信息： logicClock 每个服务器会维护一个自增的整数，名为logicClock，它表示这是该服务器发起的第多少轮投票 state 当前服务器的状态 self_id 当前服务器的myid self_zxid 当前服务器上所保存的数据的最大zxid vote_id 被推举的服务器的myid vote_zxid 被推举的服务器上所保存的数据的最大zxid 5、投票流程自增选举轮次ZooKeeper规定所有有效的投票都必须在同一轮次中。每个服务器在开始新一轮投票时，会先对自己维护的logicClock进行自增操作。 初始化选票每个服务器在广播自己的选票前，会将自己的投票箱清空。该投票箱记录了所收到的选票。例：服务器2投票给服务器3，服务器3投票给服务器1，则服务器1的投票箱为(2, 3), (3, 1), (1, 1)。票箱中只会记录每一投票者的最后一票，如投票者更新自己的选票，则其它服务器收到该新选票后会在自己票箱中更新该服务器的选票。 发送初始化选票每个服务器最开始都是通过广播把票投给自己。 接收外部投票服务器会尝试从其它服务器获取投票，并记入自己的投票箱内。如果无法获取任何外部投票，则会确认自己是否与集群中其它服务器保持着有效连接。如果是，则再次发送自己的投票；如果否，则马上与之建立连接。 判断选举轮次收到外部投票后，首先会根据投票信息中所包含的logicClock来进行不同处理： 外部投票的logicClock大于自己的logicClock。说明该服务器的选举轮次落后于其它服务器的选举轮次，立即清空自己的投票箱并将自己的logicClock更新为收到的logicClock，然后再对比自己之前的投票与收到的投票以确定是否需要变更自己的投票，最终再次将自己的投票广播出去。 外部投票的logicClock小于自己的logicClock。当前服务器直接忽略该投票，继续处理下一个投票。 外部投票的logickClock与自己的相等。当时进行选票PK。 选票PK选票PK是基于(self_id, self_zxid)与(vote_id, vote_zxid)的对比： 外部投票的logicClock大于自己的logicClock，则将自己的logicClock及自己的选票的logicClock变更为收到的logicClock 若logicClock一致，则对比二者的vote_zxid，若外部投票的vote_zxid比较大，则将自己的票中的vote_zxid与vote_myid更新为收到的票中的vote_zxid与vote_myid并广播出去，另外将收到的票及自己更新后的票放入自己的票箱。如果票箱内已存在(self_myid, self_zxid)相同的选票，则直接覆盖 若二者vote_zxid一致，则比较二者的vote_myid，若外部投票的vote_myid比较大，则将自己的票中的vote_myid更新为收到的票中的vote_myid并广播出去，另外将收到的票及自己更新后的票放入自己的票箱 统计选票如果已经确定有过半服务器认可了自己的投票（可能是更新后的投票），则终止投票。否则继续接收其它服务器的投票。 更新服务器状态投票终止后，服务器开始更新自身状态。若过半的票投给了自己，则将自己的服务器状态更新为LEADING，否则将自己的状态更新为FOLLOWING。 集群启动领导选举1、初始投票给自己集群刚启动时，所有服务器的logicClock都为1，zxid都为0。各服务器初始化后，都投票给自己，并将自己的一票存入自己的票箱，如下图所示。 在上图中，(1, 1, 0)第一位数代表投出该选票的服务器的logicClock，第二位数代表被推荐的服务器的myid，第三位代表被推荐的服务器的最大的zxid。由于该步骤中所有选票都投给自己，所以第二位的myid即是自己的myid，第三位的zxid即是自己的zxid。 此时各自的票箱中只有自己投给自己的一票。 2、更新选票服务器收到外部投票后，进行选票PK，相应更新自己的选票并广播出去，并将合适的选票存入自己的票箱，如下图所示。 服务器1收到服务器2的选票（1, 2, 0）和服务器3的选票（1, 3, 0）后，由于所有的logicClock都相等，所有的zxid都相等，因此根据myid判断应该将自己的选票按照服务器3的选票更新为（1, 3, 0），并将自己的票箱全部清空，再将服务器3的选票与自己的选票存入自己的票箱，接着将自己更新后的选票广播出去。此时服务器1票箱内的选票为(1, 3)，(3, 3)。 同理，服务器2收到服务器3的选票后也将自己的选票更新为（1, 3, 0）并存入票箱然后广播。此时服务器2票箱内的选票为(2, 3)，(3, ,3)。 服务器3根据上述规则，无须更新选票，自身的票箱内选票仍为（3, 3）。 服务器1与服务器2更新后的选票广播出去后，由于三个服务器最新选票都相同，最后三者的票箱内都包含三张投给服务器3的选票。 3、根据选票确定角色根据上述选票，三个服务器一致认为此时服务器3应该是Leader。因此服务器1和2都进入FOLLOWING状态，而服务器3进入LEADING状态。之后Leader发起并维护与Follower间的心跳。 Follower重启选举1、Follower重启投票给自己Follower重启，或者发生网络分区后找不到Leader，会进入LOOKING状态并发起新的一轮投票。 2、发现已有Leader后成为Follower服务器3收到服务器1的投票后，将自己的状态LEADING以及选票返回给服务器1。服务器2收到服务器1的投票后，将自己的状态FOLLOWING及选票返回给服务器1。此时服务器1知道服务器3是Leader，并且通过服务器2与服务器3的选票可以确定服务器3确实得到了超过半数的选票。因此服务器1进入FOLLOWING状态。 Leader重启选举1、Follower发起新投票 Leader（服务器3）宕机后，Follower（服务器1和2）发现Leader不工作了，因此进入LOOKING状态并发起新的一轮投票，并且都将票投给自己。 2、广播更新选票服务器1和2根据外部投票确定是否要更新自身的选票。这里有两种情况： 服务器1和2的zxid相同。例如在服务器3宕机前服务器1与2完全与之同步。此时选票的更新主要取决于myid的大小 服务器1和2的zxid不同。在旧Leader宕机之前，其所主导的写操作，只需过半服务器确认即可，而不需所有服务器确认。换句话说，服务器1和2可能一个与旧Leader同步（即zxid与之相同）另一个不同步（即zxid比之小）。此时选票的更新主要取决于谁的zxid较大 在上图中，服务器1的zxid为11，而服务器2的zxid为10，因此服务器2将自身选票更新为（3, 1, 11），如下图所示。 3、选出新Leader经过上一步选票更新后，服务器1与服务器2均将选票投给服务器1，因此服务器2成为Follower，而服务器1成为新的Leader并维护与服务器2的心跳。 4、旧Leader恢复后发起选举旧的Leader恢复后，进入LOOKING状态并发起新一轮领导选举，并将选票投给自己。此时服务器1会将自己的LEADING状态及选票（3, 1, 11）返回给服务器3，而服务器2将自己的FOLLOWING状态及选票（3, 1, 11）返回给服务器3。如下图所示。 5、旧Leader成为Follower服务器3了解到Leader为服务器1，且根据选票了解到服务器1确实得到过半服务器的选票，因此自己进入FOLLOWING状态。 Commit过的数据不丢失1、Failover前状态为更好演示Leader Failover过程，本例中共使用5个ZooKeeper服务器。A作为Leader，共收到P1、P2、P3三条消息，并且Commit了1和2，且总体顺序为P1、P2、C1、P3、C2。根据顺序性原则，其它Follower收到的消息的顺序肯定与之相同。其中B与A完全同步，C收到P1、P2、C1，D收到P1、P2，E收到P1，如下图所示。 这里要注意： 由于A没有C3，意味着收到P3的服务器的总个数不会超过一半，也即包含A在内最多只有两台服务器收到P3。在这里A和B收到P3，其它服务器均未收到P3 由于A已写入C1、C2，说明它已经Commit了P1、P2，因此整个集群有超过一半的服务器，即最少三个服务器收到P1、P2。在这里所有服务器都收到了P1，除E外其它服务器也都收到了P2 2、选出新Leader旧Leader也即A宕机后，其它服务器根据上述FastLeaderElection算法选出B作为新的Leader。C、D和E成为Follower且以B为Leader后，会主动将自己最大的zxid发送给B，B会将Follower的zxid与自身zxid间的所有被Commit过的消息同步给Follower，如下图所示。 在上图中： P1和P2都被A Commit，因此B会通过同步保证P1、P2、C1与C2都存在于C、D和E中 P3由于未被A Commit，同时幸存的所有服务器中P3未存在于大多数据服务器中，因此它不会被同步到其它Follower 3、通知Follower可对外服务同步完数据后，B会向D、C和E发送NEWLEADER命令并等待大多数服务器的ACK（下图中D和E已返回ACK，加上B自身，已经占集群的大多数），然后向所有服务器广播UPTODATE命令。收到该命令后的服务器即可对外提供服务。 未Commit过的消息对客户端不可见在上例中，P3未被A Commit过，同时因为没有过半的服务器收到P3，因此B也未Commit P3（如果有过半服务器收到P3，即使A未Commit P3，B会主动Commit P3，即C3），所以它不会将P3广播出去。 具体做法是，B在成为Leader后，先判断自身未Commit的消息（本例中即P3）是否存在于大多数服务器中从而决定是否要将其Commit。然后B可得出自身所包含的被Commit过的消息中的最小zxid（记为min_zxid）与最大zxid（记为max_zxid）。C、D和E向B发送自身Commit过的最大消息zxid（记为max_zxid）以及未被Commit过的所有消息（记为zxid_set）。B根据这些信息作出如下操作： 如果Follower的max_zxid与Leader的max_zxid相等，说明该Follower与Leader完全同步，无须同步任何数据 如果Follower的max_zxid在Leader的(min_zxid，max_zxid)范围内，Leader会通过TRUNC命令通知Follower将其zxid_set中大于Follower的max_zxid（如果有）的所有消息全部删除 上述操作保证了未被Commit过的消息不会被Commit从而对外不可见。 上述例子中Follower上并不存在未被Commit的消息。但可考虑这种情况，如果将上述例子中的服务器数量从五增加到七，服务器F包含P1、P2、C1、P3，服务器G包含P1、P2。此时服务器F、A和B都包含P3，但是因为票数未过半，因此B作为Leader不会Commit P3，而会通过TRUNC命令通知F删除P3。如下图所示。 本节总结 由于使用主从复制模式，所有的写操作都要由Leader主导完成，而读操作可通过任意节点完成，因此ZooKeeper读性能远好于写性能，更适合读多写少的场景 虽然使用主从复制模式，同一时间只有一个Leader，但是Failover机制保证了集群不存在单点失败（SPOF）的问题 ZAB协议保证了Failover过程中的数据一致性 服务器收到数据后先写本地文件再进行处理，保证了数据的持久性 本节将结合实例演示了使用ZooKeeper实现分布式锁与领导选举的原理与具体实现方法。 ZooKeeper节点类型ZooKeeper 提供了一个类似于 Linux 文件系统的树形结构。该树形结构中每个节点被称为 znode ，可按如下两个维度分类： 1、Persist vs. Ephemeral Persist节点，一旦被创建，便不会意外丢失，即使服务器全部重启也依然存在。每个 Persist 节点即可包含数据，也可包含子节点 Ephemeral节点，在创建它的客户端与服务器间的 Session 结束时自动被删除。服务器重启会导致 Session 结束，因此 Ephemeral 类型的 znode 此时也会自动删除 2、Sequence vs. Non-sequenceNon-sequence节点，多个客户端同时创建同一 Non-sequence 节点时，只有一个可创建成功，其它匀失败。并且创建出的节点名称与创建时指定的节点名完全一样 Sequence节点，创建出的节点名在指定的名称之后带有10位10进制数的序号。多个客户端创建同一名称的节点时，都能创建成功，只是序号不同 ZooKeeper语义保证ZooKeeper简单高效，同时提供如下语义保证，从而使得我们可以利用这些特性提供复杂的服务。 顺序性：客户端发起的更新会按发送顺序被应用到 ZooKeeper 上 原子性：更新操作要么成功要么失败，不会出现中间状态 单一系统镜像：一个客户端无论连接到哪一个服务器都能看到完全一样的系统镜像（即完全一样的树形结构）。注：根据上文《ZooKeeper架构及FastLeaderElection机制》介绍的 ZAB 协议，写操作并不保证更新被所有的 Follower 立即确认，因此通过部分 Follower 读取数据并不能保证读到最新的数据，而部分 Follwer 及 Leader 可读到最新数据。如果一定要保证单一系统镜像，可在读操作前使用 sync 方法。 可靠性：一个更新操作一旦被接受即不会意外丢失，除非被其它更新操作覆盖 最终一致性：写操作最终（而非立即）会对客户端可见 ZooKeeper Watch机制所有对 ZooKeeper 的读操作，都可附带一个 Watch 。一旦相应的数据有变化，该 Watch 即被触发。 Watch 有如下特点： 主动推送：Watch被触发时，由 ZooKeeper 服务器主动将更新推送给客户端，而不需要客户端轮询。 一次性：数据变化时，Watch 只会被触发一次。如果客户端想得到后续更新的通知，必须要在 Watch 被触发后重新注册一个 Watch。 可见性：如果一个客户端在读请求中附带 Watch，Watch 被触发的同时再次读取数据，客户端在得到 Watch 消息之前肯定不可能看到更新后的数据。换句话说，更新通知先于更新结果。 顺序性：如果多个更新触发了多个 Watch ，那 Watch 被触发的顺序与更新顺序一致。 分布式锁与领导选举关键点1、最多一个获取锁 / 成为Leader对于分布式锁（这里特指排它锁）而言，任意时刻，最多只有一个进程（对于单进程内的锁而言是单线程）可以获得锁。 对于领导选举而言，任意时间，最多只有一个成功当选为Leader。否则即出现脑裂（Split brain） 2、锁重入 / 确认自己是Leader对于分布式锁，需要保证获得锁的进程在释放锁之前可再次获得锁，即锁的可重入性。 对于领导选举，Leader需要能够确认自己已经获得领导权，即确认自己是Leader。 3、释放锁 / 放弃领导权锁的获得者应该能够正确释放已经获得的锁，并且当获得锁的进程宕机时，锁应该自动释放，从而使得其它竞争方可以获得该锁，从而避免出现死锁的状态。 领导应该可以主动放弃领导权，并且当领导所在进程宕机时，领导权应该自动释放，从而使得其它参与者可重新竞争领导而避免进入无主状态。 4、感知锁释放 / 领导权的放弃当获得锁的一方释放锁时，其它对于锁的竞争方需要能够感知到锁的释放，并再次尝试获取锁。 原来的Leader放弃领导权时，其它参与方应该能够感知该事件，并重新发起选举流程。 非公平领导选举从上面几个方面可见，分布式锁与领导选举的技术要点非常相似，实际上其实现机制也相近。这里以领导选举为例来说明二者的实现原理，分布式锁的实现原理也几乎一致。 1、选主过程假设有三个ZooKeeper的客户端，如下图所示，同时竞争Leader。这三个客户端同时向ZooKeeper集群注册Ephemeral且Non-sequence类型的节点，路径都为 /zkroot/leader（工程实践中，路径名可自定义）。 如上图所示，由于是Non-sequence节点，这三个客户端只会有一个创建成功，其它节点均创建失败。此时，创建成功的客户端（即上图中的Client 1）即成功竞选为 Leader 。其它客户端（即上图中的Client 2和Client 3）此时匀为 Follower。 2、放弃领导权如果 Leader 打算主动放弃领导权，直接删除 /zkroot/leader 节点即可。 如果 Leader 进程意外宕机，其与 ZooKeeper 间的 Session 也结束，该节点由于是Ephemeral类型的节点，因此也会自动被删除。 此时 /zkroot/leader 节点不复存在，对于其它参与竞选的客户端而言，之前的 Leader 已经放弃了领导权。 3、感知领导权的放弃由上图可见，创建节点失败的节点，除了成为 Follower 以外，还会向 /zkroot/leader 注册一个 Watch ，一旦 Leader 放弃领导权，也即该节点被删除，所有的 Follower 会收到通知。 4、重新选举感知到旧 Leader 放弃领导权后，所有的 Follower 可以再次发起新一轮的领导选举，如下图所示。 从上图中可见： 新一轮的领导选举方法与最初的领导选举方法完全一样，都是发起节点创建请求，创建成功即为 Leader，否则为 Follower ，且 Follower 会 Watch 该节点 新一轮的选举结果，无法预测，与它们在第一轮选举中的顺序无关。这也是该方案被称为非公平模式的原因 小结 非公平模式实现简单，每一轮选举方法都完全一样 竞争参与方不多的情况下，效率高。每个 Follower 通过 Watch 感知到节点被删除的时间不完全一样，只要有一个 Follower 得到通知即发起竞选，即可保证当时有新的 Leader 被选出 给ZooKeeper 集群造成的负载大，因此扩展性差。如果有上万个客户端都参与竞选，意味着同时会有上万个写请求发送给 Zookeper。如《ZooKeeper架构》一文所述，ZooKeeper 存在单点写的问题，写性能不高。同时一旦 Leader 放弃领导权，ZooKeeper 需要同时通知上万个 Follower，负载较大。 公平领导选举1、选主过程如下图所示，公平领导选举中，各客户端均创建 /zkroot/leader 节点，且其类型为Ephemeral与Sequence。 由于是Sequence类型节点，故上图中三个客户端均创建成功，只是序号不一样。此时，每个客户端都会判断自己创建成功的节点的序号是不是当前最小的。如果是，则该客户端为 Leader，否则即为 Follower。 在上图中，Client 1创建的节点序号为 1 ，Client 2创建的节点序号为 2，Client 3创建的节点序号为3。由于最小序号为 1 ，且该节点由Client 1创建，故Client 1为 Leader 。 2、放弃领导权Leader 如果主动放弃领导权，直接删除其创建的节点即可。 如果 Leader 所在进程意外宕机，其与 ZooKeeper 间的 Session 结束，由于其创建的节点为Ephemeral类型，故该节点自动被删除。 3、感知领导权的放弃与非公平模式不同，每个 Follower 并非都 Watch 由 Leader 创建出来的节点，而是 Watch 序号刚好比自己序号小的节点。 在上图中，总共有 1、2、3 共三个节点，因此Client 2 Watch /zkroot/leader1，Client 3 Watch /zkroot/leader2。（注：序号应该是10位数字，而非一位数字，这里为了方便，以一位数字代替） 一旦 Leader 宕机，/zkroot/leader1 被删除，Client 2可得到通知。此时Client 3由于 Watch 的是 /zkroot/leader2 ，故不会得到通知。 重新选举重新选举Client 2得到 /zkroot/leader1 被删除的通知后，不会立即成为新的 Leader 。而是先判断自己的序号 2 是不是当前最小的序号。在该场景下，其序号确为最小。因此Client 2成为新的 Leader 。 这里要注意，如果在Client 1放弃领导权之前，Client 2就宕机了，Client 3会收到通知。此时Client 3不会立即成为Leader，而是要先判断自己的序号 3 是否为当前最小序号。很显然，由于Client 1创建的 /zkroot/leader1 还在，因此Client 3不会成为新的 Leader ，并向Client 2序号 2 前面的序号，也即 1 创建 Watch。该过程如下图所示。 小结 实现相对复杂； 扩展性好，每个客户端都只 Watch 一个节点且每次节点被删除只须通知一个客户端； 旧 Leader 放弃领导权时，其它客户端根据竞选的先后顺序（也即节点序号）成为新 Leader，这也是公平模式的由来； 延迟相对非公平模式要高，因为它必须等待特定节点得到通知才能选出新的 Leader。 本节总结基于 ZooKeeper 的领导选举或者分布式锁的实现均基于 ZooKeeper 节点的特性及通知机制。充分利用这些特性，还可以开发出适用于其它场景的分布式应用。]]></content>
      <categories>
        <category>zookeeper</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[搜索系统实现]]></title>
    <url>%2F2017%2F12%2F20%2Fsearch-system%2F</url>
    <content type="text"><![CDATA[业务场景随着公司业务的发展，数据量的不断增长，以前各业务线对于搜索数据维护的能力显得有些捉襟见肘。同时由于早期没有进行架构方面的规划，业务线各自维护es作为搜索引擎，随之而来的问题如单点、版本不统一、没有很好的规划分片数据等，对于高可用以及实时响应带来了挑战，同时也增加了运维成本。 目前有些数据搜索依旧在数据库进行的，随着数据量增加以及查询维度的提升，单个数据库的读写能力有限，未来存在分片的可能性，对数据一片片查询，对数据库返回结果在内存聚合的方式，缺点显而易见，同时增加了开发的复杂性，对数据库的性能也带来了不必要的负担。 基于上述的原因，遂决定建立通用的搜索服务来为业务线提供统一的入口。同时提供了高可用、以及良好的响应时间。 随之而来的问题数据更新是一个比较重要的问题，之前解决的方案是每次有数据修改，同时修改对应记录的sync_status状态，然后由定时任务扫描sync_status，将修改记录同步到es。定时任务存在的问题： sync_status = 0查询会进行全表扫描，即使创建索引效率不会有大的提升 定时任务频率问题，同步频率过低，会造成数据可搜索窗口期过大，频率过高，加上全表扫描会对数据库带来过大的压力 减少对业务的影响，要做到零耦合，不侵入现有业务。 当变更数据量比较大的时候，写入es效率限制，可能会导致数据变更到可索引的窗口期增大。 针对同一条数据的修改，可能存在较早修改覆盖最新修改的可能，所以针对一条数据的修改应该是串行的。 开源搜索引擎的选择因为之前使用的es搜索引擎，所以在选择上更偏向于该方案。同时也做了简单的调研。 查看了主流的Java搜索引擎方案，主要是solr和es。具体的比较网上文章比较多，就不多做介绍，我总结了我选择es的几点： es更轻量级、更易用，安装、配置方便，不需要依赖第三方包 可以根据服务器数量调整分片位置，只需要前期根据数据量增长，指定合理的分片数 跟其他如logstash、kibana等方便组合，后续可以作为日志平台 最终方案最终经过考虑采用了如下图的解决方案: 为了解决定时轮询扫表带来的数据库压力，我们通过定于mysql的binlog，我们采用阿里的canal框架，订阅响应的表的binlog，同样也做到了对业务的零侵入。 解析binlog之后，引入mq来解决上游流量过大给下游服务带来的压力。同时我们es的批量写入功能，防止过多的单个请求。 对于数据覆盖问题，暂时的想法是，记录某条记录的最后修改时间，这个在项目上线后看下效果，是否对性能有影响。 项目采用CQRS架构，将读、写服务分离，将业务复杂性放到Read Server中，Write Server负责数据修改、删除，同时方便以后更好扩展。Read Server对外提供Rest接口，通过Eureka进行客户端负载均衡。Write Server负责消费MQ中的消息进行解析，将数据保存到ES。 总结该方案仅为初期设计方案，可能还需要逐步演进。后面出现的问题，会进行更新，该方案也仅供参考。]]></content>
      <categories>
        <category>实例</category>
      </categories>
      <tags>
        <tag>架构</tag>
        <tag>搜索</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[贫血领域模型]]></title>
    <url>%2F2017%2F12%2F20%2Fanemic-domain-model%2F</url>
    <content type="text"><![CDATA[这是已经存在了很长一段时间的反模式之一，但目前似乎正在发生特别的迸发。我们都注意到他们似乎越来越受欢迎。作为一个合适的领域模型的推动者，这不是一件好事。 一个贫血的领域模型的基本症状是，乍一看它看起来像真实的东西。有一些对象，许多以域空间中的名词命名，并且这些对象与真实域模型具有的丰富关系和结构相关联。当你观察这些行为的时候，你会发现这些东西几乎没有任何行为，它们只是一堆的getter和setter。事实上，这些模型经常带有设计规则，表明您不应该将域逻辑放入域对象中。取而代之的是一组服务对象，它们捕获所有的域逻辑，执行所有的计算，并使用结果更新模型对象。这些服务位于领域模型的顶部，并使用域模型作为数据。 这种反模式的根本恐怖之处在于它与面向对象设计的基本思想相悖；也就是将数据和过程结合在一起。贫血的领域模型实际上只是一个过程式的设计，就像我（和Eric）一样，自我们在Smalltalk的早期就一直在斗争的东西。更糟糕的是，许多人认为贫血的对象是真正的对象，因此完全忽略了面向对象设计的要点。 现在，面向对象的语言是很好的，但是我意识到我需要更多的基本论点来反对这种贫血。实质上，贫血领域模型的问题在于它们承担了领域模型的所有成本，而没有产生任何好处。主要成本是对数据库的映射的笨拙，通常会导致整个O/R映射。如果您使用强大的面向对象技术来组织复杂的逻辑，这是值得的。然而，通过将所有行为都导出到服务中，基本上会以事务脚本结束，从而失去域模型带来的好处。 同样值得强调的是，将行为放到域对象中不应该与使用分层来将域逻辑与持久化和表示职责分开的可靠方法相矛盾。应该在域对象中的逻辑是域逻辑 - 验证，计算，业务规则 - 无论你喜欢怎么称呼它。（有些情况下，当您在域对象中放置数据源或表示逻辑时发生争论，但这与我对贫血症的观点正交。） 所有这一切的混乱之处在于，许多面向对象的专家建议在域模型之上建立一层程序服务，以形成服务层。但这并不是一个使域模型无效的参数，实际上服务层主张使用一个服务层与一个行为丰富的域模型相结合。 Eric Evans的优秀的书籍领域驱动设计有以下几层。 应用层[服务层名称]:定义软件应该做的工作，并指导表达域对象解决问题。该层负责的任务对业务有意义或者与其他系统的应用层进行&gt; 交互所必需的。这层保持薄。它不包含业务规则或知识，但只负责协调任务，并将任务委托给下一层的域对象的协作。它没有反映业务情&gt; 况的状态，但它可以具有反映用户或程序的任务进度的状态。 域层（或模型层）：负责代表业务的概念、业务情况的信息和业务规则。反映业务情况的状态在这里被控制和使用，即使存储它的技术细节被委托给基础设施。这一层是商业软件的核心。这里的关键点是服务层很薄——所有的关键逻辑都在域层中。他在他的服务模式中重申了这一点： 现在，更常见的错误是过于轻易地将行为融入适当的对象，逐渐滑向过程式编程。 我不知道为什么这种反模式如此普遍。我怀疑这是由于许多人没有真正使用合适的领域模型，特别是如果他们来自数据背景。一些技术鼓励它；比如J2EE的Entity Beans，这是我更喜欢POJO领域模型的原因之一。一般来说，您在服务中发现的行为越多，就越有可能抢夺领域模型的好处。如果你所有的逻辑都在服务中，那么你已经失明了。 POJO首字母缩略词: 普通旧式Java对象（Plain Old Java Object）。 在谈话中，我们指出了将业务逻辑编码为普通Java对象而不是使用Entity Beans的许多益处。我们想知道为什么人们反对在他们的系统中使用常规对象，并得出结论，这是因为简单的对象缺乏一个奇特的名字。所以我们给了他们一个，并且它很好地被抓住了。]]></content>
      <categories>
        <category>DDD</category>
      </categories>
      <tags>
        <tag>DDD</tag>
        <tag>分层</tag>
        <tag>翻译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CQRS]]></title>
    <url>%2F2017%2F12%2F07%2Fcqrs%2F</url>
    <content type="text"><![CDATA[CQRS代表Command Query Responsibility Segregation(命令查询职责分离)。它的核心思想是，您可以使用不同的模型来更新信息，而不是使用您用来读取信息的模型。在某些情况下，这种分离是有价值的，但是要注意，对于大多数系统CQRS会增加风险的复杂性。 人们用于与信息系统交互的主流方法是将其视为CRUD数据存储。我的意思是，我们有一些记录结构的构思模型，我们可以创建新的记录，读取记录，更新已有的记录，并在我们完成这些记录时删除记录。在最简单的情况下，我们的交互都是关于存储和检索这些记录的。 随着我们的需求变得更加复杂，我们逐渐远离这种模式。我们可能希望以不同于记录存储的方式查看信息，可能会将多个记录合并为一个记录，或者通过组合不同位置的信息来形成虚拟记录。在更新方面，我们可能会发现验证规则，只允许存储特定的数据组合，或者甚至可能推断要存储的数据与我们提供的数据不同。 当这发生时，我们开始看到信息的多重表示。当用户与信息进行交互时，他们会使用各种各样的信息，每种信息都是不同的表示形式。开发人员通常会构建自己的概念模型，用它来操纵模型的核心元素。如果你使用的是域模型，那么这通常是域的概念表示。您通常也会将持久性存储尽可能地接近概念模型。 这种多层表示的结构可能会变得非常复杂，但是当人们这样做时，他们仍然将其解析为单一的概念表示，这充当了所有表示之间的概念集成点。 CQRS引入的变化是将该概念模型分解为独立的更新和显示模型，分别称为Command和Query，遵循CommandQuerySeparation的词汇表。其基本原理是，对于许多问题，特别是在更复杂的领域中，对于命令和查询拥有相同的概念模型，会导致一个更复杂的模型，但这两者都不太好。 通过单独的模型，我们通常意味着不同的对象模型，可能运行在不同的逻辑过程中，可能在不同的硬件上。Web示例将会看到用户正在查看使用查询模型呈现的网页。如果他们发起更改，将更改发送到单独的命令模型进行处理，则生成的更改将传递给查询模型以呈现更新后的状态。 这里有相当大的变化空间。内存中的模型可以共享相同的数据库，在这种情况下，数据库充当两个模型之间的通信。但是，它们也可能使用单独的数据库，从而有效地将查询端的数据库变为实时ReportingDatabase。在这种情况下，需要在两个模型或其数据库之间建立一些通信机制。 这两个模型可能不是单独的对象模型，可能是相同的对象在其命令端和其查询端具有不同的接口，而与关系数据库中的视图相似。但通常当我听说CQRS时，它们显然是分开的模型。 CQRS自然适合其他一些架构模式。 当我们从一个通过CRUD操作的单一表示方式离开时，我们可以很容易地转移到基于任务的UI。 CQRS非常适合基于事件的编程模型。 通常将CQRS系统拆分为与事件协作通信的单独服务。这使得这些服务能够很容易地利用事件的资源。 单独的模型引发了如何难以保持这些模型一致的问题，这提高了使用最终一致性的可能性。 对于许多领域而言，更新时需要大部分逻辑，因此使用EagerReadDerivation来简化查询方模型可能很有意义。 如果写模型为所有更新生成事件，则可以将读模型构造为EventPosters，从而使它们成为MemoryImages，从而避免大量数据库交互。 CQRS适用于复杂领域，这种领域也受益于领域驱动设计。 何时使用它像任何模式一样，CQRS在一些地方是有用的，但在其他地方则不是。许多系统都适合CRUD智能模型，所以应该以这种风格来完成。CQRS对于所有相关的人来说是一个重大的精神飞跃，所以不应该被解决，除非这个好处是值得的。尽管我已经遇到了CQRS的成功使用，但迄今为止，我遇到的大多数情况都不是那么好，CQRS被看作是使软件系统陷入严重困难的重要力量。 特别是CQRS只能用于系统的特定部分（DDD术语中的BoundedContext），而不是整个系统。以这种思维方式，每个有界上下文都需要自己决定如何对其进行建模。 到目前为止，我看到了两个方面的好处。 首先是一些复杂的领域可能更容易通过使用CQRS来解决。然而，我必须强调，CQRS的这种适用性在很大程度上是少数情况。通常，命令和查询之间有足够的重叠，共享模型更容易。在与其不匹配的域上使用CQRS会增加复杂性，从而降低生产力并增加风险。 另一个主要好处是处理高性能应用程序。CQRS允许您将读取和写入的负载分开，允许您独立地扩展。如果您的应用程序在读写之间看到很大的差异，这非常方便。即使没有这一点，您可以对两边应用不同的优化策略。一个例子是使用不同的数据库访问技术来读取和更新。 如果您的域不适合CQRS，但是您需要的查询会增加复杂性或性能问题，请记住，您仍然可以使用ReportingDatabase。CQRS为所有查询使用一个单独的模型。通过报告数据库，您仍然可以将主要系统用于大多数查询，但可以将要求更高的数据转移到报告数据库。 尽管有这些好处，你应该对使用CQRS非常谨慎。许多信息系统与信息库的概念非常吻合，它的更新方式与读取的方式相同，将CQRS添加到这样的系统中会增加相当的复杂性。我确实看到过一些案例，它对生产力造成了很大的拖累，给项目增加了不必要的风险，即使是在有能力的团队的手中。因此，尽管CQRS是一种很好的模式，但要注意，它很难使用，如果你处理不当，你可以很容易地砍掉重要的部分。]]></content>
      <categories>
        <category>DDD</category>
      </categories>
      <tags>
        <tag>DDD</tag>
        <tag>分层</tag>
        <tag>翻译</tag>
        <tag>架构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[dubbo暴露公网IP]]></title>
    <url>%2F2017%2F11%2F25%2Fdubbo-expose-public-network%2F</url>
    <content type="text"><![CDATA[最近需要搭建一套测试环境，主要是协助开发进行跨系统联调。目前系统间调用使用的dubbo框架，测试服务器是在云上，而不是局域网内。这样测试环境就带来两个严重的问题 环境隔离 dubbo默认注册一般使用本地地址，当你连接测试环境zk，获取的是云服务器的内网地址，本地是无法调通的 关于环境隔离问题，比如开发A注册自己的服务到测试环境的zk，这样带来的问题你的服务已经暴露给测试环境服务，测试环境的rpc调用会轮训到你注册的服务，而你的服务注册到zk的是本地局域网地址，测试环境肯定找不到这个地址，所以调用失败。关于dubbo暴露ip问题详见 怎么解决这个问题呢？dubbo提供了只订阅功能，不会把本地服务注册到zk上，这样就解决了上面描述的问题。 1&lt;dubbo:registry address="zookeeper://127.0.0.1:2181" register="false" /&gt; 随之而来的问题，我们不将我们服务注册到zk，那我们本地需要调用这个服务的消费者怎么进行rpc调用呢？同样dubbo为我们提供了直连机制 1&lt;dubbo:reference id="xxxService" interface="com.alibaba.xxx.XxxService" url="dubbo://localhost:20890" /&gt; 在xml中配置的话，可能会比较繁琐，我们可以统一在配置文件中配置，使用方式详见 这样我们就解决了环境隔离问题，测试环境不会发现我们本地服务，同样也可以使用测试环境服务进行测试了。 那么接下来看第二个问题，正常情况下我们没办法访问到，测试环境的内网服务，毕竟不是一个局域网内。那么我们自能让服务给我暴露公网ip，我们建立连接。 网上比较多的方式是修改/etc/hosts，这种方案在我们使用kerberos认证的时候会有问题。可以使用dubbo:protocol的host来实现。 1&lt;dubbo:protocol name="dubbo" port="20880" host="123.x.x.x" /&gt; 现在我们的问题就都解决了。。。 通过xml配置register=false和host=123.x.x.x，对于代码侵入比较大。可以通过-D参数进行配置：dubbo.protocol.host=123.x.x.x。]]></content>
      <categories>
        <category>dubbo</category>
      </categories>
      <tags>
        <tag>dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mybatis使用enum]]></title>
    <url>%2F2017%2F11%2F20%2Fmybaits-use-enum%2F</url>
    <content type="text"><![CDATA[项目中经常会遇到一些PO类中的某个字段的取值范围范围是一组固定的值，为了方便期间使用了number来存储这些值。这样会带来的一个问题，当我创建一个PO对象时，这个字段怎么赋值，应该有那些值？通常我们会提供一个enum来维护这个字段的取值范围，随之而来的问题当项目中enum越来越多，或者文档没有更新的情况下，开发人员很难知道，这个字段是不是有enum与之对应，或者是哪个enum与之对应。很多时候我们知道有几种值，我们随手就赋值了一个字面常量值。这就可能出现了莫名其妙的类型。最直接的解决方案，我们在PO字段上直接声明enum类型，就完全避开了错误使用值的情况。 MyBatis官网提供了两种方案，EnumTypeHandler和EnumOrdinalTypeHandler。EnumTypeHandler在数据库中使用VARCHAR存储enum的名字。EnumOrdinalTypeHandler在数据库使用NUMBER或DOUBLE存储enum的索引。 我们更想的是在表中存储NUMBER类型，所以EnumOrdinalTypeHandler更适合我们的需求。EnumOrdinalTypeHandler存储是enum的索引，但是如果我们在enum中想使用自己的code，而不是enum本身的索引，要怎么去做呢？根据MyBatis官网文档我们可以自定义类型处理器来试下，下面是自定义处理器的使用方法。 既然我们的目的是想使用自定义的code存储到表中，所以首先我们应该强制我们的enum类型应该有对应的code字段，我们通过在enum中实现接口来实现。 123456789public interface EnumCode &#123; /** * 返回枚举值代码 * * @return */ int getCode();&#125; 上面我们定义了一个接口EnumCode它值定义了一个方法就是返回code。在我们的enum中实现该接口 123456789101112131415161718192021222324252627282930public enum PlatformEnum implements EnumCode &#123; /** * 安卓 */ ANDROID(0), /** * IOS */ IOS(1), /** * 微信小游戏 */ WECHAT(2), /** * H5 */ H5(3); private int code; PlatformEnum(int code) &#123; this.code = code; &#125; @Override public int getCode() &#123; return code; &#125;&#125; 接下来就是实现类型处理器，代码如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class EnumCodeTypeHandler extends BaseTypeHandler&lt;EnumCode&gt; &#123; private Class&lt;EnumCode&gt; type; public EnumCodeTypeHandler(@NonNull Class&lt;EnumCode&gt; type) &#123; this.type = type; &#125; @Override public void setNonNullParameter(PreparedStatement ps, int i, EnumCode parameter, JdbcType jdbcType) throws SQLException &#123; ps.setByte(i, (byte) parameter.getCode()); &#125; @Override public EnumCode getNullableResult(ResultSet rs, String columnName) throws SQLException &#123; int i = rs.getByte(columnName); if (rs.wasNull()) &#123; return null; &#125; else &#123; return getValuedEnum(i); &#125; &#125; @Override public EnumCode getNullableResult(ResultSet rs, int columnIndex) throws SQLException &#123; int code = rs.getByte(columnIndex); return rs.wasNull() ? null : getValuedEnum(code); &#125; @Override public EnumCode getNullableResult(CallableStatement cs, int columnIndex) throws SQLException &#123; int code = cs.getByte(columnIndex); return cs.wasNull() ? null : getValuedEnum(code); &#125; /** * 转换为&#123;@link EnumCode&#125; * * @param value * @return */ private EnumCode getValuedEnum(int value) &#123; EnumCode[] codes = type.getEnumConstants(); for (EnumCode em : codes) &#123; if (em.getCode() == value) &#123; return em; &#125; &#125; throw new IllegalArgumentException( "Cannot convert " + value + " to " + type.getSimpleName() + " by value."); &#125;&#125; 以上就完成我们自定义的类型处理器。接下来就看看我们如果在xml中使用类型处理器： 1234567&lt;resultMap&gt; &lt;result column="platform" jdbcType="TINYINT" property="platform" typeHandler="cn.finegames.commons.integrate.EnumCodeTypeHandler"/&gt;&lt;/resultMap&gt; &lt;if test="platform != null"&gt; #&#123;platform,jdbcType=TINYINT, typeHandler=cn.finegames.commons.integrate.EnumCodeTypeHandler&#125;,&lt;/if&gt; 在声明resultMap和进行插入、修改时使用上面形式就可以了。 还有一种比较常用的情况，如果在where语句中使用enum，而不是通过方法传入值该怎么处理？这种情况我也搞了很久，查了很多资料，终于找到对应的方案… 1token_type = $&#123;@cn.xxx.TokenTypeEnum@ACCESS.getCode()&#125; 使用OGNL表达式就可以了。]]></content>
      <categories>
        <category>mybatis</category>
      </categories>
      <tags>
        <tag>mybaits</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring Boot使用yml格式cron表达式]]></title>
    <url>%2F2017%2F11%2F18%2Fspring-cron-with-yml%2F</url>
    <content type="text"><![CDATA[通常在spring工程中我们会将定时任务的cron表达式配置在properties中，当项目迁移到spring boot下时，spring boot提供了更为便利的yml配置文件。关于yml的使用以及优势我们不做过多的讨论。本篇文章讨论下在yml中配置cron程序的使用问题。 因为我们使用yml文件所以之前类似于@Schedule(${app.cron})肯定是行不通的，yml配置根本不支持这种形式。这就比较难办了，我们怎么才能再次有效的使用el表达式赋值给我们的cron字段呢？ 首先为了能获得到该值，我们需要定义一个类，并使用成员变量来接收这些值。 例如我们的yml文件内容如下： 12app: cron: 0 0 12 * * ? 我们定义一个Java Config的类来接收该配置 123456789101112131415@Configurable@ConfigurationProperties("app")public class CronConfig &#123; private String cron; @Bean public String cron() &#123; return this.cron; &#125; public void setCron(String cron) &#123; this.cron = cron; &#125;&#125; 重点就是在我们的@Bean上。这样我们就可以在我们的定时任务cron表达式使用该值了。 1234567public class MySchedule &#123; @Schedule(cron="#&#123;@cron&#125;") public void execute() &#123; &#125;&#125; 感觉是麻烦了一些😆，也可以使用properties、yml结合的方式。]]></content>
      <categories>
        <category>spring boot</category>
      </categories>
      <tags>
        <tag>spring boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[spring cloud maven配置]]></title>
    <url>%2F2017%2F11%2F17%2Fspring-cloud-maven-config%2F</url>
    <content type="text"><![CDATA[spring cloud maven的配置有两种方式，第一种是官网提供的配置，项目的pom文件继承spring-boot、并且在dependencyManager添加spring cloud依赖，这种方式的使用了maven的继承，如果我们的项目有自己的parent pom，就出现问题了。因此我们可以使用另一种方式，先看下两种配置方式： 第一种方式 123456789101112131415161718192021222324252627282930&lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;1.3.5.RELEASE&lt;/version&gt; &lt;relativePath /&gt; &lt;!-- lookup parent from repository --&gt;&lt;/parent&gt;&lt;dependencyManagement&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;Brixton.RELEASE&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/dependencyManagement&gt;&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-config&lt;/artifactId&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 第二种实现方式，需要在我们的parent pom中进行如下的配置： 1234567891011121314151617&lt;dependencyManagement&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-dependencies&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.boot.version&#125;&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-dependencies&lt;/artifactId&gt; &lt;version&gt;$&#123;spring.cloud.version&#125;&lt;/version&gt; &lt;type&gt;pom&lt;/type&gt; &lt;scope&gt;import&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencyManagement&gt; 我们继承该pom，如果想使用eureka模块，只需要添加该模块的依赖 123456&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.cloud&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-starter-netflix-eureka-server&lt;/artifactId&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 如果需要使用spring-boot-maven-plugin插件，也需要在parent pom中进行配置。 123456789101112131415161718&lt;build&gt; &lt;pluginManagement&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;version&gt;$&#123;spring-boot.version&#125;&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;repackage&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt;&lt;/build&gt; 在项目的pom中添加如下配置 12345678&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt;]]></content>
      <categories>
        <category>spring cloud</category>
      </categories>
      <tags>
        <tag>spring cloud</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[nexus 3.x上传本地jar]]></title>
    <url>%2F2017%2F11%2F11%2Fnexus3-upload-local-jar%2F</url>
    <content type="text"><![CDATA[今天碰到需要上传非maven管理的jar到nexus，记录下。语法如下： 12345678mvn deploy:deploy-file -DgroupId=&lt;group-id&gt;\-DartifactId=&lt;artifact-id&gt;\-Dversion=&lt;version&gt;\-Dpackaging=&lt;type-of-packaging&gt;\-Dfile=&lt;path-to-file&gt;\-DrepositoryId=&lt;id-to-map-on-server-section-of-settings.xml&gt;\-Durl=&lt;url-of-the-repository-to-deploy&gt; 举一个简单的栗子： 1234567mvn deploy:deploy-file -DgroupId=lt.jave -DartifactId=jave -Dversion=1.0.2 -Dfile=jave-1.0.2.jar -DrepositoryId=nexus -Durl=http://localhost:8081/repository/maven-releases/ DrepositoryId=nexus指的是setting配置的验证的id]]></content>
      <categories>
        <category>nexus</category>
      </categories>
      <tags>
        <tag>nexus</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[redis异常汇总]]></title>
    <url>%2F2017%2F10%2F30%2Fredis-exception%2F</url>
    <content type="text"><![CDATA[zmalloc.h:50:31: 致命错误：jemalloc/jemalloc.h：没有那个文件或目录在进入redis目录进行过make时，会出现该异常。解决方案：make MALLOC=libc CentOS 7下配置Redis开机启动首先在官网下载源码包，解压后进入该目录执行make install。redis相关命令会存在于/usr/local/bin/下。 启动脚本在redis home下的utils目录下：redis_init_script。 12345678910111213141516171819202122232425262728293031323334353637#!/bin/sh#chkconfig: 2345 80 90#description:auto_run## Simple Redis init.d script conceived to work on Linux systems# as it does use of the /proc filesystem.REDISPORT=6379EXEC=/usr/local/bin/redis-serverCLIEXEC=/usr/local/bin/redis-cliPIDFILE=/var/run/redis_$&#123;REDISPORT&#125;.pidCONF="/etc/redis/$&#123;REDISPORT&#125;.conf"case "$1" in start) if [ -f $PIDFILE ] then echo "$PIDFILE exists, process is already running or crashed" else echo "Starting Redis server..." $EXEC $CONF fi ;; stop) if [ ! -f $PIDFILE ] then echo "$PIDFILE does not exist, process is not running" else PID=$(cat $PIDFILE) echo "Stopping ..." $CLIEXEC -p $REDISPORT shutdown while [ -x /proc/$&#123;PID&#125; ] do echo "Waiting for Redis to shutdown ..." sleep 1 done echo "Redis stopped" fi ;; *) echo "Please use start or stop as first argument" ;;esac 可以看到配置文件中指定的端口号REDISPORT为默认的6379；redis-server命令的全路径 EXEC；以及配置文件的路径CONF。我们将该文件复制到/etc/init.d目录下 将配置文件放到CONF所在的目录/etc/redis下。 设置开机启动：chkconfig redis on]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>exception</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AMQP 0-9-1模型解释]]></title>
    <url>%2F2017%2F10%2F22%2Famqp-0-9-1-model-explained%2F</url>
    <content type="text"><![CDATA[本指南提供了AMQP 0-9-1协议的概述，该协议是RabbitMQ支持的协议之一。 AMQP 0-9-1AMQP 0-9-1(高级消息队列协议)是一种消息传递协议，它使符合标准的客户端应用程序能够与符合标准的消息传递中间件代理进行通信。 代理和他们的规则消息传递代理接收来自发布者的消息(发布它们的应用程序，也称为生产者)，并将它们发送给消费者(处理它们的应用程序)。 由于它是一种网络协议，发布者、消费者和代理都可以在不同的机器上。 AMQP 0-9-1模型简述AMQP 0-9-1模型具有以下的世界观：信息被发布到交换(exchange)，经常被比作邮局或邮箱。交换使用称为绑定的规则将消息副本分发到队列。然后AMQP代理交付消息到订阅队列的消费者，或消费者根据需要从队列拉取消息。 当发布消息时，发布者可以指定各种消息属性(消息元数据)。一些元数据可能会被代理使用，但其余的对于代理来说是完全不透明的，并且仅接收消息的应用程序使用。 网络是不可靠的，应用程序可能无法处理消息，因此AMQP模型有一个消息确认的概念。当消息被交付到消费者时，消费者既可以自动，同样应用开发者也可以选择手动通知代理。当使用消息确认时，代理只会在接收到该消息(或一组消息)的通知时，才会完全删除队列中的消息。 在某些情况下，例如，当一个消息不能被路由(routed)，消息可能会被返回给发布者，放弃，或如果代理实现了扩展，则将其放入所谓的“dead letter queue”中。发布者通过发布消息使用某些参数来选择如何处理这样的情况。 队列(queue)、交换(exchange)和绑定(binding)统称为AMQP实体。 AMQP是一种可编程的协议AMQP 0-9-1是一种可编程的协议，从某种意义上说，AMQP 0-9-1实体和路由方案主要由应用程序自己定义，而不是代理管理员。因此，为声明队列、交换，定义他们之间的绑定，订阅队列等协议操作制定了条款。 这给了应用程序开发人员很大的自由，但同时也要求他们注意潜在的定义冲突。在实践中，定义冲突是罕见的，而通常表明错误配置。 应用程序声明它们需要的AMQP 0-9-1实体，定义必要的路由方案，并在不再使用它们时，可以选择删除AMQP 0-9-1实体。 交换和交换类型交换是消息发送的AMQP实体。交换会获取到一个消息，路由到零个或多个队列。使用的路由算法取决于交换类型和称为绑定的规则。AMQP 0-9-1代理提供了四种交换类型： 类型 默认预先声明的名字 Direct exchange 空字符串和 amq.direct Fanout exchange amq.fanout Topic exchange amq.topic Headers exchange amq.match 和amq.headers 处理交换类型，交换可以声明许多属性，其中最重要的是： Name Durability - 代理重启时交换存活下来 Auto-delete - 当最后的一个队列解绑交换将会删除 Arguments - 可选的，被插件和代理特定的功能使用 交换可以是持久的也可以是瞬态的。持久的交换会在代理重启存活下来，然而瞬态的交换则不会（当代理重新上线时必须重新声明）。并非所有的场景和用例都要求交换是持久的。 默认的交换默认的交换是由代理预先声明没有名字（空字符串）的direct交换。它有一个特殊的属性使它对简单的应用程序非常有用：创建的每个队列都将使用与队列名称相同的路由键自动绑定到它。 例如，当你声明一个search-indexing-online的队列，AMQP 0-9-1代理将会使用search-indexing-online作为路由键绑定到默认的交换。因此，一个消息使用路由键search-indexing-online发布到默认交换，将会被路由到队列search-indexing-online。换句话说，换句话说，默认的交换使它看起来可以直接向队列发送消息，尽管从技术上讲这并不是正在发生的事情。 可以在rabbitmq的default-exchange中找到对应的示例。 Direct交换Direct交换是基于消息的routing key交付消息到队列。Direct交换是对消息单播路由的理想选择(尽管它们也可以用于多播路由)。下面是它的工作原理： 队列使用路由键K绑定到交换 当使用路由键R的新消息到达直接交换时，如果K = R，交换器会将其路由到队列中。 Direct交换通常用来以一种循环的方式在多个woker之间分配任务（同一个应用的实例）。在这样做的时候，重要的一点是，在AMQP 0-9-1中，消息是在消费者之间，而不是在队列之间进行负载均衡。 Direct交换可以用图形方式表示如下: Fanout交换一个fanout交换路由消息到所有绑定到它的队列，而路由键被忽略。如果N个队列被绑定到一个fanout交换，当一个新的消息被发布到该交换时，一个消息的副本将被发送到所有的N队列。Fanout交换是消息的广播路由的理想选择。 因为一个fanout交换器将一个消息的副本传递给每个队列，所以它的用例非常相似: 大型多人在线游戏(MMO)可以使用它来进行排行榜更新或其他全局事件 体育新闻网站可以使用fanout交换，近实时地向移动客户分发比分更新 分布式系统可以广播各种状态和配置更新 群组聊天可以在使用fanout交换的参与者之间分发消息(尽管AMQP没有一个内置的概念存在，所以XMPP可能是一个更好的选择)。 一个fanout交换可以用图形方式表示如下: Topic交换主题交换基于消息路由键和用于将队列绑定到交换的模式之间的匹配，将消息路由到一个或多个队列。主题交换类型通常用于实现各种发布/订阅模式的变化。主题交换通常用于消息的多播路由。 Topic交换有一个非常广泛的用例。当一个问题涉及多个消费者/应用程序，它们有选择性地选择它们想要什么类型的消息时，应该考虑使用主题交换。 使用示例： 发布与特定地理位置相关的数据，例如销售点 由多个woker完成的后台任务处理，每一个都能够处理特定的任务集 股票价格更新(以及其他金融数据的更新) 包含分类或标记的新闻更新(例如，只针对特定的体育或团队) 云计算服务的编制 分布式架构/特定于OS的软件构建或打包，每个构建器只能处理一个架构或操作系统 Headers交换headers交换被设计用于在多个属性上进行路由，这些属性更容易表示为消息头，而不是路由键。Headers交换忽略路由键属性。相反，用于路由的属性是从header属性中获取的。如果消息头的值等于绑定上指定的值，那么消息就被认为是匹配的。 可以使用多个头进行匹配将一个队列绑定到headers交换。在这种情况下，代理需要从应用程序开发人员获得更多信息，也就是说，它是否应该考虑消息与任何头匹配，或者所有的？这就是“x-match”绑定参数的用途。当“x-match”参数被设置为“any”时，只有一个匹配的头值就足够了。或者，将“x-match”设置为“all”的命令，所有的值必须匹配。 Headers交换可以被看作是“Direct交换的”。因为它们是基于header值进行路由的，它们可以被用作直接的交换，路由键不必是字符串;例如，它可以是一个整数或哈希(字典)。 队列AMQP 0-9-1模型中的队列与其他消息和任务排队系统中的队列非常相似；他们存储被应用程序消费的消息。队列可以与交换共享一些属性，但也有一些附加属性: Name Durable - 代理重启存活 Exclusive - 仅能被一个连接使用，当连接关闭队列将被删除 Auto-delete - 队列至少有一个消费者如果最后一个消费者取消订阅将被删除 Arguments - 可选的；被插件和代理特定的功能使用，例如新消息TTL，队列长度限制等。 在使用队列之前必须要声明。队列的声明会在队列不存在的情况下会创建。如果队列已经存在，并且其属性与声明中的属性相同，则声明将没有影响。当现有队列属性与声明中不同，一个通道级异常(PRECONDITION_FAILED)代码406将抛出。 队列名称应用程序可以选择一个队列名称或者要求代理为它们声明一个。队列名称最多可以达到255字节的utf-8字符。AMQP 0-9-1代理可以为应用程序生成一个惟一的队列名称。要使用此功能，请将空字符串作为队列名称参数传递。生成的名称和队列声明的响应返回给客户端。 以”amq.”开头的队列名称留作代理内部使用。尝试使用违反该规则的名称声明队列将会导致一个channel级别的异常回复代码403(ACCESS_REFUSED)。 队列持久性持久队列被持久化到磁盘中，因此可以在代理重新启动时存活。非持久的队列我们成为瞬态。并不是所有的场景和用例都要求队列是持久的。 队列的持久性不会使被路由到该队列的消息持久。如果代理被取下，然后重新启动，在代理启动时将重新声明持久队列，但是只会恢复持久性消息。 绑定(Binding)绑定是交换用来路由消息到队列的规则。要指示交换E将消息发送到队列Q，Q必须绑定到E。绑定可能有一些交换类型使用的可选的路由键属性。路由键的目的是选择发布到交换的某些消息，以被路由到绑定队列。换句话说，路由键就像过滤器一样。 描述一个类比： 队列就像你在纽约的目的地 交换箱JFK机场 绑定是从JFK到你的目的地的路线。可以有零或多种途径 有了这个间接层允许不能实现或非常难实现的直接发送到队列的路由场景 ，也可以消除一些应用程序开发人员必须要做，重复的工作。 如果AMQP消息不能被路由到任何队列(例如，因为没有对其发布的交换的绑定)，那么它要么被丢弃，要么返回给发布者，这取决于发布者所设置的消息属性。 消费者在队列中存储消息是没有用的，除非应用程序消费它们。在AMQP 0-9-1模型中，应用程序有两种方法: 将消息交付给他们（Push API） 根据需要获取信息（Pull API） 使用“push API”，应用程序必须表明对来自特定队列的消息的兴趣。当他们这样做的时候，我们说他们注册了一个消费者，或者简单地说，订阅了一个队列。每个队列有一个以上的消费者，或者注册一个独占的消费者(将所有其他的消费者排除在队列之外)。 每个消费者(订阅)都有一个称为消费者标签(consumer tag)的标识符。它可以用于从消息中取消订阅。消费者标签只是字符串。 消息确认消费者应用程序—接收和处理消息的应用程序—可能偶尔会处理个别的消息失败，或者有时会崩溃。网络问题也有可能造成问题。这就提出了一个问题:AMQP代理何时应该从队列中删除消息?amqp 0-9-1规范提出了两种选择: 代理向应用程序发送一条消息后(使用basic.deliver或basic.get-ok AMQP方法)。 应用程序发送回确认后（使用basic.ack AMQP方法） 前者被称为自动确认模型，后者被称为显式的确认模型。使用显式模型，应用程序选择什么时候发送一个确认。它可能是在收到一条信息后，或者在处理之前将它持久化到一个数据仓库中，或者在完全处理消息之后（例如，成功地获取一个Web页面，处理并将其存储到某个持久的数据存储中）。 如果一个消费者在没有发送消息的情况下死亡，AMQP代理将把它重新交付给另一个消费者，或者，如果当时没有可用的服务，在尝试重新提交之前，代理将等待至少一个用户注册到相同的队列中。 拒绝消息当使用者应用程序接收到消息时，该消息的处理可能会成功，也可能不会成功。一个应用程序可以通过拒绝一个消息，向代理表明消息处理已经失败(或者不能在当时完成)。当拒绝消息时，应用程序可以要求代理放弃或重新请求它。当队列中只有一个消费者时，请确保您不会通过拒绝并重新排队(requeue)使用同一消费者的消息来创建无限的消息传递循环。 否定的（Negative ）确认使用AMQP basic.reject方法拒绝消息。basic.reject有一个限制：没有办法拒绝多个消息，虽然您可以使用确认这么做。但是，如果使用RabbitMQ，则有一个解决方案。RabbitMQ提供了一个AMQP 0-9-1扩展，称为确否定确认或nacks。更多的信息，请参考帮助页。 预取消息对于多个消费者共享一个队列的情况，在发送下一个确认消息之前，可以指定每个消费者可以同时发送多少条消息是有用的。这可以作为一种简单的负载平衡技术，或者如果消息倾向于批量发布，则可以提高吞吐量。例如，如果一个生产应用程序每分钟都发送消息，这是因为它所做的工作的性质。 请注意，RabbitMQ只支持Channel级别的预取计数，而不是基于Connection或基于大小的预取。 消息属性和有效载荷（Payload）AMQP模型中的消息具有属性。有些属性非常常见，AMQP 0-9-1规范定义了它们，应用程序开发人员不必考虑确切的属性名。一些示例： 内容类型 内容编码 路由键 交付模式（持续的或不） 消息优先级 消息发布时间戳 过期期限 发布者应用id AMQP代理使用了一些属性，但大多数都是对接受它们的应用程序开放的。有些属性是可选的，并且被认为是header。他们类似于HTTP中的X-Headers。消息属性是在消息发布时设置的。 AMQP消息也有一个有效负载(它们携带的数据)，AMQP代理将其视为一个不透明的字节数组。代理将不检查或修改有效负载。消息只包含属性和没有有效负载是可能的。常见的是使用序列化格式，如JSON、Thrift、Protocol Buffer和MessagePack来序列化结构化数据，以便将其作为消息有效负载发布。AMQP对等点通常使用“内容类型”和“内容编码”字段来传达这些信息，但这仅仅是惯例。 消息可能以持久的形式发布，这使得AMQP代理将它们持久化到磁盘中。如果服务器重新启动，系统将确保接收到的持久消息不会丢失。简单地将消息发布到持久的交换中，或者将其路由到持久的队列(s)的事实并不会使消息持久:这完全取决于其本身的持久性模式。将消息发布为持久的影响性能(就像数据存储一样，持久性在性能上是有代价的)。 消息确认由于网络是不可靠的，应用程序也会失败，所以通常需要有某种类型的处理确认。有时只需要承认已经收到消息的事实。有时确认意味着消息经过消费者的验证和处理，例如，被验证为具有强制数据，并持久化到一个数据存储或索引。 这种情况非常常见，所以AMQP 0-9-1有一个称为消息确认的内置功能(有时称为“ack”)，用户可以使用它来确认消息的传递和/或处理。如果应用程序崩溃(AMQP代理在连接关闭时注意到这一点)，如果对一条消息的确认是预期的，而AMQP代理没有接收到，则消息将重新排队(如果存在，则可能立即传递给另一个使用者)。 在协议中内置了确认信息，可以帮助开发人员构建更健壮的软件。 AMQP 0-9-1 方法AMQP 0-9-1是由许多方法构成的。方法是操作(比如HTTP方法)，与面向对象编程语言中的方法没有什么共同之处。AMQP方法被集合到类中。类只是AMQP方法的逻辑分组。AMQP 0-9-1参考资料提供了所有AMQP方法的详细信息。 让我们来看看交换类，这是一组与交换操作有关的方法。它包括以下操作: exchange.declare exchange.declare_ok exchange.delete exchange.delete_ok (请注意，RabbitMQ站点参考还包括对exchange类的特定扩展，这些扩展是我们在本指南中不会讨论的)。 上面的操作是逻辑对的；exchange.declare和exchange.declare-ok，exchange.delete和exchange.delete-ok。这些操作是“请求”(由客户端发送)和“响应”(由代理响应上述的“请求”)。 例如，客户端请求代理使用exchange.declare声明一个新的交换。声明方法: 如上图所示，exchange.declare携带了几个参数。可以使客户端能够指定交换名称、类型、持久性标志等等。 如果操作成功，代理使用exchange.declare-ok方法来响应： exchange.declare-ok除了通道号(channel number)，不会携带任何参数。 事件的发生顺序与AMQP queue类的另一个方法对，queue-declare和queue-declare-ok非常相似。 并非所有的AMQP方法都有对应的方法。一些（basic.publish作为最广泛使用的一种）没有对应的的“响应”方法和其他（例如，basic.get)有不止一种可能的“响应”。 连接AMQP连接通常是长期存在的。AMQP是一种应用程序级协议，它使用TCP进行可靠的传递。AMQP连接使用身份验证，可以使用TLS(SSL)保护。当应用程序不再需要连接到AMQP代理时，它应该优雅地关闭AMQP连接，而不是突然关闭底层的tcp连接。 通道一些应用程序需要多个AMQP代理连接。但是，同时保持许多TCP连接是不可取的，因为这样做会消耗系统资源，并且使配置防火墙变得更加困难。AMQP 0-9-1连接是多路复用的通道，可以被认为是“共享一个TCP连接的轻重量连接”。 对于使用多个线程/进程进行处理的应用程序，在每个线程/进程中打开一个新的通道，而不是在它们之间共享通道是很常见的。 特定通道上的通信完全独立于另一个通道上的通信，因此每个AMQP方法也都包含一个通道号，客户端使用该通道来确定该方法的哪个通道(例如，需要调用哪个事件处理程序)。 虚拟主机（virtual host)为了使单个代理能够承载多个隔离的“环境”(用户、交换、队列等)，AMQP包含了虚拟主机(vhost)的概念。它们类似于许多受欢迎的Web服务器使用的虚拟主机，并提供了完全隔离的环境，在这些环境中AMQP实体可以生存。AMQP客户机在AMQP连接协商中指定它们想要使用的vhost。 AMQP是可扩展的AMQP 0-9-1有几个扩展点: 自定义交换类型允许开发人员实现路由方案，交换类型提供的开箱即用的类型不能很好地覆盖，例如基于地理数据的路由。 交换和队列的声明可以包括代理可以使用的附加属性。例如，在RabbitMQ中，每个队列的消息TTL都是这样实现的。 对协议的特定于代理的扩展。例如扩展RabbitMQ实现。 新的AMQP 0-9-1方法类被引入 可以使用扩展的插件扩展代理，例如，RabbitMQ管理前端和HTTP API是作为一个插件实现的。 这些特性使AMQP 0-9-1模型更加灵活，适用于非常广泛的问题。 PS: 以上所展示AMQP中相应概念的代码已在我的github。]]></content>
      <categories>
        <category>rabbitmq</category>
      </categories>
      <tags>
        <tag>翻译</tag>
        <tag>amqp</tag>
        <tag>rabbitmq</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java内存模型]]></title>
    <url>%2F2017%2F09%2F14%2FJava-Memory-Model%2F</url>
    <content type="text"><![CDATA[Java内存模型指定Java虚拟机如何与计算机的内存（RAM）一起工作。Java虚拟机是整个计算机的模型，因此该模型自然包括一个内存模型 - 也就是Java内存模型。 如果要设计正确的并发程序，了解Java内存模型非常重要。Java内存模型指定不同线程如何以及何时可以看到其他线程写入共享变量的值，以及如何在必要时同步对共享变量的访问。原始Java内存模型不足，因此Java内存模型在Java 1.5中进行了修订。 此版本的Java内存模型仍在Java 8中使用。 JVM的内部JVM内部使用的Java内存模型在线程堆栈和堆之间划分内存。下图从逻辑角度说明了Java内存模型： 在Java虚拟机中运行的每个线程都有自己的线程堆栈。线程堆栈包含有关线程调用哪些方法以达到当前执行点的信息。我将其称为“调用堆栈”。当线程执行其代码时，调用堆栈会发生变化。 线程堆栈还包含正在执行的每个方法的所有局部变量（调用堆栈上的所有方法）。线程只能访问它自己的线程堆栈。由线程创建的局部变量对于除创建它的线程之外的所有其他线程是不可见的。即使两个线程正在执行完全相同的代码，两个线程仍将在每个自己的线程堆栈中创建该代码的局部变量。因此，每个线程都有自己的每个局部变量的版本。原始类型的所有局部变量（boolean，byte，short，char，int，long，float，double）完全存储在线程堆栈中，因此对其他线程不可见。一个线程可以将一个原始变量的副本传递给另一个线程，但它不能共享原始局部变量本身。 堆包含在Java应用程序中创建的所有对象，无论创建该对象的线程是什么。包括原始类型的对象版本（例如Byte，Integer，Long等）。无论是创建对象并将其分配给局部变量，还是创建为另一个对象的成员变量，该对象仍然存储在堆上。下面的图表说明了存储在线程堆栈上的调用堆栈和局部变量，以及存储在堆上的对象： 局部变量可以是基本类型，在这种情况下，它完全保留在线程堆栈上。 局部变量也可以是对象的引用。在这种情况下，引用（局部变量）存储在线程堆栈中，但是对象本身存储在堆上。 对象可能包含方法，这些方法可能包含局部变量。 即使方法所属的对象存储在堆上，这些局部变量也存储在线程堆栈中。 对象的成员变量与对象本身一起存储在堆上。当成员变量是基本类型时，以及它是对象的引用时都是如此。 静态类变量也与类定义一起存储在堆上。 所有具有对象引用的线程都可以访问堆上的对象。当一个线程有权访问一个对象时，它也可以访问该对象的成员变量。如果两个线程同时在同一个对象上调用一个方法，它们都可以访问该对象的成员变量，但每个线程都有自己的局部变量副本。 这是一个说明上述要点的图表： 两个线程有一组局部变量。其中一个局部变量（局部变量2）指向堆上的共享对象（对象3）。两个线程各自对同一对象具有不同的引用。它们的引用是局部变量，因此存储在每个线程的线程堆栈中。但是，这两个不同的引用指向堆上的同一个对象。 注意共享对象（对象3）如何将对象2和对象4作为成员变量引用（由对象3到对象2和对象4的箭头所示）。通过对象3中的这些成员变量引用，两个线程可以访问对象2和对象4。 该图还显示了一个局部变量，该变量指向堆上的两个不同对象。在这种情况下，引用指向两个不同的对象（对象1和对象5），而不是同一个对象。理论上，如果两个线程都引用了两个对象，则两个线程都可以访问对象1和对象5。但是在上图中，每个线程只引用了两个对象中的一个。 那么，什么样的Java代码可以导致上面的内存图？好吧，代码就像下面的代码一样简单： 12345678910111213141516171819202122232425262728293031public class MyRunnable implements Runnable() &#123; public void run() &#123; methodOne(); &#125; public void methodOne() &#123; int localVariable1 = 45; MySharedObject localVariable2 = MySharedObject.sharedInstance; //... do more with local variables. methodTwo(); &#125; public void methodTwo() &#123; Integer localVariable1 = new Integer(99); //... do more with local variable. &#125;&#125;public class MySharedObject &#123; //static variable pointing to instance of MySharedObject public static final MySharedObject sharedInstance = new MySharedObject(); //member variables pointing to two objects on the heap public Integer object2 = new Integer(22); public Integer object4 = new Integer(44); public long member1 = 12345; public long member2 = 67890;&#125; 如果两个线程正在执行run()方法，那么前面显示的图表将是结果。run()方法调用methodOne()，methodOne调用methodTwo。 methodOne()声明一个原始局部变量（类型为int的localVariable1）和一个对象引用局部变量（localVariable2）。 执行methodOne()的每个线程将在各自的线程堆栈上创建自己的localVariable1和localVariable2副本。localVariable1变量将彼此完全分离，仅存在于每个线程的线程堆栈中。一个线程无法看到另一个线程对其localVariable1副本所做的更改。 执行methodOne()的每个线程也将创建自己的localVariable2副本。但是，localVariable2的两个不同副本最终都指向堆上的同一个对象。代码将localVariable2设置为指向静态变量引用的对象。只有一个静态变量的副本，并且此副本存储在堆上。因此，localVariable2的两个副本最终都指向静态变量指向的MySharedObject的同一个实例。MySharedObject实例也存储在堆上。 它对应于上图中的Object 3。 注意MySharedObject类如何包含两个成员变量。成员变量本身与对象一起存储在堆上。两个成员变量指向另外两个Integer对象。这些Integer对象对应于上图中的Object 2和Object 4。 另请注意methodTwo()如何创建名为localVariable1的局部变量。此局部变量是对Integer对象的对象引用。该方法将localVariable1引用设置为指向新的Integer实例。localVariable1引用将存储在执行methodTwo()的每个线程的一个副本中。实例化的两个Integer对象将存储在堆上，但由于该方法每次执行都会创建一个新的Integer对象，因此执行此方法的两个线程将创建单独的Integer实例。在methodTwo()中创建的Integer对象对应于上图中的Object 1和Object 5。 还要注意类型为long的类MySharedObject中的两个成员变量，它们是基本类型。由于这些变量是成员变量，因此它们仍与对象一起存储在堆上。 只有局部变量存储在线程堆栈中。 硬件内存结构现代硬件内存架构与内部Java内存模型略有不同。了解硬件内存架构也很重要，以了解Java内存模型如何与其一起工作。本节介绍了常见的硬件内存架构，后面的部分将介绍Java内存模型如何与其配合使用。 这是现代计算机硬件架构的简化图： 现代计算机通常有2个或更多CPU。其中一些CPU也可能有多个内核。关键是，在具有2个或更多CPU的现代计算机上，可以同时运行多个线程。每个CPU都能够在任何给定时间运行一个线程。这意味着如果您的Java应用程序是多线程的，则每个CPU的一个线程可能同时（并发地）在Java应用程序中运行。 每个CPU包含一组基本上在CPU内存中的寄存器。CPU可以在这些寄存器上执行的操作比在主存储器中对变量执行的操作快得多。这是因为CPU可以比访问主存储器更快地访问这些寄存器。 每个CPU还可以具有CPU缓存存储层。事实上，大多数现代CPU都有一些大小的缓存存储层。CPU可以比主存储器更快地访问其高速缓冲存储器，但通常不会像访问其内部寄存器那样快。因此，CPU高速缓存存储器介于内部寄存器和主存储器的速度之间。某些CPU可能有多个缓存层（一级和二级），但要了解Java内存模型如何与内存交互，这一点并不重要。重要的是要知道CPU可以有某种缓存存储层。 计算机还包含主存储区（RAM）。 所有CPU都可以访问主内存。 主存储器区域通常比CPU的高速缓存存储器大得多。 通常，当CPU需要访问主存储器时，它会将部分主存储器读入其CPU缓存。它甚至可以将部分缓存读入其内部寄存器，然后对其执行操作。当CPU需要将结果写回主存储器时，它会将值从其内部寄存器刷新到高速缓冲存储器，并在某些时候将值刷新回主存储器。 当CPU需要在缓存存储器中存储其他东西时，存储在高速缓存存储器中的值通常被刷回到主存储器。CPU缓存可以一次将数据写入其内存的一部分，并一次刷新部分内存。它不必在每次更新时读/写完整缓存。通常，缓存在称为“缓存行”的较小存储块中更新。可以将一个或多个高速缓存行读入高速缓冲存储器，并且可以再次将一个或多个高速缓存行刷新回主存储器。 弥补Java内存模型和硬件内存体系结构之间的差距如前所述，Java内存模型和硬件内存架构是不同的。硬件内存架构不区分线程堆栈和堆。在硬件上，线程堆栈和堆都位于主存储器中。线程堆栈和堆的一部分有时可能存在于CPU高速缓存和内部CPU寄存器中。这在图中说明: { % java-memory-model-5.png java memory model % } 当对象和变量可以存储在计算机的各种不同存储区域中时，可能会出现某些问题。两个主要问题是： 线程更新（写入）对共享变量的可见性。 读取，检查和写入共享变量时的竞争条件。 以下各节将解释这两个问题。 共享对象的可见性如果两个或多个线程共享一个对象，而没有正确使用volatile声明或同步，则一个线程对共享对象的更新可能对其他线程不可见。 想象一下，共享对象最初存储在主存储器中。然后，在CPU上运行的线程将共享对象读入其CPU缓存中。它在那里对共享对象进行了更改。只要CPU缓存未刷新回主内存，共享对象的更改版本对于在其他CPU上运行的线程是不可见的。这样，每个线程最终都可以拥有自己的共享对象副本，每个副本都位于不同的CPU缓存中。 下图说明了描述的情况。左侧CPU上运行的一个线程将共享对象复制到其CPU缓存中，并将其count变量更改为2。在右侧的CPU上运行的其他线程看不到此更改，因为尚未将计数更新刷新回主内存。 要解决此问题，您可以使用Java的volatile关键字。volatile关键字可以确保直接从主内存读取给定变量，并在更新时始终写回主内存。 竞态条件如果两个或多个线程共享一个对象，并且多个线程更新该共享对象中的变量，则可能发生竞争条件。 想象一下，如果线程A将共享对象的变量count读入其CPU缓存中。线程B做同样的事情，但进入不同的CPU缓存。现在，线程A将count加1，而线程B执行相同的操作。现在var1已经增加了两次，每个CPU缓存一次。 如果这些增量按顺序执行，则变量计数将增加两次，并将原始值+ 2写回主存储器。 但是，两个增量同时执行而没有适当的同步。无论将其更新版本的计数写回主存储器的线程A和B中的哪一个，更新的值将仅比原始值高1，尽管有两个增量。 该图说明了如上所述的竞争条件问题的发生： 要解决此问题，您可以使用Java synchronized块。同步块保证在任何给定时间只有一个线程可以进入代码的给定关键部分。同步块还保证在同步块内访问的所有变量都将从主内存读入，当线程退出同步块时，无论变量是否声明为volatile，所有更新的变量都将再次刷回主内存。 文章翻译自]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>翻译</tag>
        <tag>java memeory model</tag>
      </tags>
  </entry>
</search>
